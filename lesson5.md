
## 5. 차원축소를 통한 $n \ll p$문제 접근

### 5.1. 인자분석

#### 5.1.1. 인자분석이란?

인자분석이란 서로 연관되어있는 다변량 데이터에서 변수 간의 조합을 통해 새로운 변수를 만들어내는 분석기법이다. 변수들의 선형합 형태로 정의되는 새로운 변수를 만들어낸다는 점에서는 주성분분석과 유사하지만 수학적인 원리는 근본적으로 차이가 있다.

인자분석은 데이터의 총변동량 중 **공통분산**과 **고유분산**을 나누어 공통분산 부분을 설명할 수 있는 **잠재변수**를 찾아내는 접근법이다. 인자분석의 아이디어는 아래와 같은 수식으로 표현할 수 있다.(다변량 분석, 김성수 등, 한국방송통신대학교출판문화원, 2017, p. 67)

```math

\boldsymbol X - \boldsymbol \mu = \boldsymbol {LF} + \boldsymbol \epsilon \\

\boldsymbol X = (X_1, X_2, \cdots, X_p)' \\

E(\boldsymbol X) = \boldsymbol \mu = (\mu_1, \mu_2, \cdots, \mu_p)' \\

L = \begin{pmatrix}

l_{11} & l_{12} & \cdots & l_{1q} \\

l_{21} & l_{22} & \cdots & l_{2q} \\

& \vdots & \\

l_{p1} & l_{p2} & \cdots & l_{pq} \\

\end{pmatrix} \\

\boldsymbol F = ( f_1, f_2, \cdots, f_q)' \\

\boldsymbol \epsilon = (\epsilon_1, \epsilon_2, \cdots, \epsilon_p)'

```

위 수식은 다음과 같이 해석할 수 있다. 각 변수에서 평균값을 빼서 중심화한 데이터($\boldsymbol {X - \mu}$)를 인자부하($\boldsymbol L$)와 인자($\boldsymbol f$)의 곱과 고유분산($\boldsymbol \epsilon$)의 합으로 분해한다. 이를 전체 데이터 세트가 아니라 개별 변수 단위에서 위 식을 이해하면 아래와 같다.

```math
X_i = l_{i1} f_1 + l_{i2} f_2 + \cdots + l_{iq} f_q + \epsilon_i
```

위 식은 일반적인 선형회귀와 매우 흡사한 형태를 가진다. 실제로 인자분석은 이론적 타당성을 담보하기 위해서 변수와 인자에 관한 아래와 같은 기본가정을 전제로 한다.

```
1. 변수들은 다변량 정규분포를 따른다.
2. 인자와 고유분산의 평균은 모두 0이고, 인자의 분산은 1이다.
3. 인자쌍과 특수인자쌍의 공분산은 모두 0이고, 인자와 고유분산은 서로 독립이다.
```

> $X \sim N(\mu, \Sigma)$  
> $E(\boldsymbol f) = 0$  
> $Cov(\boldsymbol F) = I_{q \times q}$  
> $E(\boldsymbol \epsilon) = 0$

그런데 고유값 분해에 기반한 주성분분석은 해의 존재성과 유일성이 보장되는 반면 임의적인 분해 기법인 인자분석은 해의 유일성이 보장되지 않는다. 특이값분해를 이용하여 인자를 구하는 경우를 예로 들어보자.(통계학으로 배우는 머신러닝, 트레버 헤이스티 등, 이판호 역, 에이콘, 2021, p. 648) 다변량 데이터 $\boldsymbol X$를 특이값분해하면 다음과 같은 행렬구성을 얻을 수 있다.

```math
\boldsymbol X = \boldsymbol {UDV'}
```

이때, $\boldsymbol S = \sqrt N \boldsymbol U$, $\boldsymbol A' = \boldsymbol {DV'} / \sqrt N$으로 정의하면 위 식은 아래와 같이 쓸 수 있다.

```math
\boldsymbol X = \boldsymbol {SA'}
```

위 식을 인자분석의 관점에서 해석하면 $\boldsymbol S$를 인자, $\boldsymbol A$를 인자부하행렬로 이해할 수 있다. 위 식의 양변에 임의의 직교행렬 $\boldsymbol R$을 곱하면 다음과 같이 정리할 수 있다.


```math
\boldsymbol X = \boldsymbol {SA}' = \boldsymbol {SR'RA'} = \boldsymbol {S^* A^*}
```

즉, 원래 구성과 다른 성분의 인자와 인자부하행렬을 이론상 무수히 많은 수만큼 만들어낼 수 있다. (자세한 내용은 [아래](#513-인자분석의-비식별성과-축회전)에서 기술)

무수히 많은 인자와 인자부하행렬 중에서 어떤 행렬을 생성할 것이냐는 문제를 단순화하기 위해서 인자 간의 직교가정(가정 3번)이다. 인자 간 직교가정으로 인해 인자의 상관계수행렬을 별도로 추정할 필요가 없어지면서 공통분산 분해의 문제를 풀기 위해 추정해야하는 행렬이 2개에서 1개로 줄어든다. (수학적인 설명이 너무 장황해지는 것을 막기 위해서 더 자세한 설명은 생략합니다. 그냥 상관계수가 고정되면 행렬3개의 곱으로 표시되는 공통분산이 동일한 행렬의 2차곱 형태로 표시할 수 있어서 추정해야하는 행렬의 수가 줄어든다고 이해하시면 됩니다.) 

이렇게 해가 될 수 있는 행렬의 범위를 좁혀놔도 어떤 행렬을 선택할 것인가는 여전히 임의적인 선택의 범위 내에 있다. 이를 수치적인 방법으로 해를 구하는 방식으로 최종적인 인자부하행렬을 구하는데, 가장 많이 사용하는 방법은 최대우도법과 주축법(principal axis method)이다. 최대우도법은 $\boldsymbol X$가 다변량 정규분포를 따른다는 가정을 이용하여 분포함수로부터 우도함수를 유도한 후 우도함수값을 최대화할 수 있는 해를 알고리즘을 이용하여 구하는 방법이다. EM 알고리즘이나 뉴턴-랩슨법을 이용한다. 주축법은 인자부하행렬의 업데이트가 기준치 이하로 이루어질 때까지 공통분산에 해당하는 행렬을 고유값분해하는 알고리즘을 반복하는 방법이다. (이 부분 역시 수학적인 배경이 너무 복잡하기 때문에 설명은 생략합니다.)

위와 같은 과정을 거쳐서 인자부하행렬을 구한 후 원변수에 대한 부하값의 차이를 통해 인자분석의 결과를 활용한다. 통상 두 인자값의 평면상에 변수값을 표시한 후 인자값에 대한 해석을 정의하는 방식을 사용한다. 

---

#### 5.1.2. 인자분석과 주성분분석의 차이점

첫째, 주성분분석은 고유값분해라는 수학적인 근거를 두고 해를 구하지만 인자분석은 다분히 임의적인 분산 분해에 기반을 둔다. 이와 관련하여 해의 유일성이 보장되지 않는 문제를 해결하기 위해 여러 가지 제약과 가정을 두고 인자행렬을 구하기 때문에 '임의성' 비판에서 자유롭지 못하다.

둘째, 주성분분석은 데이터가 가지고 있는 총분산을 분해하는 분석 방법인 반면 인자분석은 데이터가 가지고 있는 총분산 중 각 변수의 상관성이 있는 부분과 없는 부분을 나누어 상관성이 있는 부분에 대해서만 분석을 한다. 이때 분산을 나누는 기준 자체도 매우 모호하기 때문에 상관성이 있는 부분의 비율을 미리 정해놓고 분석을 진행한다.

셋째, 주성분분석은 분석에 포함할 성분의 개수를 정하는 과정에서 정보의 손실이 발생하지만 인자분석은 분석 과정에서 자연스럽게 정보의 손실이 발생한다. 이때, 특이값분해에 기반하여 공통분산과 고유분산을 분해하는 주축법을 사용하면 결과가 주성분분석과 매우 유사하게 나온다.

---

#### 5.1.3. 인자분석의 비식별성과 축회전

인자 간의 독립성 조건이 없는 상태에서 인자분석 모형은 일반적으로 아래와 같은 식으로 표시한다. (의미상으로는 [위](#511-인자분석이란)에서 설명한 것과 같습니다.)

```math
\boldsymbol \Sigma = \boldsymbol { \Lambda \Phi \Lambda' + \Psi }
```

이때, 임의의 ($m \times m$) 차원의 가역행렬(역행렬이 존재하는 행렬) $\boldsymbol T$를 이용하면 $\boldsymbol { \Lambda \Phi \Lambda' }$를 아래와 같이 변환할 수 있다.

```math
\boldsymbol {\Lambda \Phi \Lambda'} = \boldsymbol { (\Lambda T) (T^{-1}\Phi T) (\Lambda T)' }
```

위와 같이 변환하면 최초 ($\boldsymbol {\Lambda, \Phi}$) 조합으로 표현되었던 인자분석의 부하행렬은 ($\boldsymbol {\Lambda T, T^{-1} \Phi T}$) 조합의 부하행렬로 바꿀 수 있다. 즉, 인자분석의 조건을 만족하는 인자부하행렬은 무수히 많은 동치값을 가지고 있는데, 이를 **인자분석의 비식별성** 또는 **식별성 문제**라고 한다. 이는 인자분석의 과학적 타당성을 약화하는 요인이기도 하다.

반면, 이러한 성질을 이용하여 직교행렬(행렬을 구성하는 열벡터의 내적이 0인 행렬, 기하학적으로는 좌표평면상에서 직각으로 교차하는 벡터라고 이해하시면 됩니다. 정의상 모든 직교행렬은 가역행렬이기 때문에 비식별성 문제에서 사용한 $\boldsymbol T$의 역할을 할 수 있습니다.)을 이용하여 인자부하행렬을 변환할 수 있는데, 이러한 분석기법을 **축회전**이라고 한다.

축회전기법은 직교행렬의 성질 혹은 변환하는 행렬이 인자 간의 상호상관을 허용할 것인가 여부에 따라 varimax(요인 간 비상관을 유지한 상태에서 인자값이 축에서 벗어나는 정도를 최소화할 수 있게 축을 회전하는 방법), promax(요인 간 상관을 허용, varimax 최적화 관점에서 접근), oblimin(요인 간 상관을 허용, 별도의 함수를 이용하여 요인 해석에 가장 적합한 축을 만들어내는 방법) 등 기법이 있다.

축회전은 인자부하의 해석을 용이하게 해준다. 회전하기 전 축에서 유사한 방향으로 변수가 모여있지만 축과 직각이 아닌 경우에는 해석이 까다로워지지만 축회전을 하고 나면 변수들이 각각의 축을 중심으로 모여있기 때문에 어떤 측면에서 변수 간의 성질이 구별되는지가 한눈에 확인이 된다. 

> 파이썬 노트북 5.1. 참조

---

### 5.2. 차원축소와 회귀분석

$n \ll p$ 데이터의 차원축소 기법을 회귀분석과 결합하여 분석하는 기법으로는 주성분회귀와 부분최소제곱회귀가 있다. 이들 기법은 $n \ll p$에서 발생하는 변수의 차원문제를 해결할 수 있다는 점에서 회귀분석과 결합하여 데이터를 분석할 수 있는 타당한 방법을 제공한다. 아래에서는 차원축소 외에 각각의 분석방법에 어떤 장단점이 있는지를 살펴본다.


#### 5.2.1. 주성분회귀(Principla Component Regression, PCR)

주성분회귀는 주성분분석을 통해 생성한 주성분 중 일부를 설명변수로 가지는 선형회귀모형을 적합하는 분석 방법이다. 

주성분회귀의 장점은 주성분분석의 특성상 주성분 간의 독립성이 보장된다는 점이다. 선형회귀분석에서 설명변수 간 높은 상관성이 낳을 수 있는 모형의 여러가지 문제를 감안하면 독립이 보장되는 설명변수 투입은 회귀모형과 잘 어울리는 조합이라고 할 수 있다. 

또 다른 장점은 설명변수가 가지는 체계적인 변동요인을 반응변수와의 관계에 반영할 수 있다는 점이다. 회귀분석의 중요한 목표 중 하나가 설명변수와 반응변수 간의 관계를 규명하는 것이기 때문에 상관관계를 포함한 반응변수 전체의 변동분을 투입변수로 배치하는 것은 회귀분석의 타당성을 높여준다.

단점은 주성분의 본질은 데이터의 변동성이기 때문에 처음 변수조합을 설정할 때에 가정했던 이론적인 연결관계가 담보되지 않는다는 점이다. 즉, 주성분은 변동성의 크기를 기준으로 총분산 중 일부분을 취하기 때문에 버리는 총변동에서 반응변수와 관련이 있는 요인이 들어있을 수 있다. 변동성이 낮은 변수의 구성비율이 큰 주성분이 낮은 고유값을 가진다면 그 주성분을 버릴 가능성이 높은데, 만약 해당 변수가 반응변수와 높은 상관관계를 가지고 있다면 전체 회귀모형이 반응변수와 설명변수 간의 관계를 적절하게 포착하지 못할 것이다.

이 점에서 주성분회귀는 분석의 목적이 예측에 맞춰져있다면 적절하지 못한 분석 방법이라고 할 수 있다.

---

#### 5.2.2. 부분최소제곱회귀(Partial Least Squares Regression, PLS)

주성분은 반응변수를 고려하지 않고 생성하기 때문에 주성분을 설명변수로 하는 회귀분석을 진행하는 경우 반응변수의 변동성이 회귀모형의 설명변수에 반영되지 않는 결함이 생긴다. 전술한 바와 같이 주성분을 선택하는 과정에서 제거한 주성분에 적재량이 많은 원변수가 있는 경우, PCA는 이 변수와 반응변수 간의 연결관계를 놓칠 수 있다. 이러한 단점을 보완하기 위해 설명변수뿐만 아니라 반응변수와의 공분산을 고려하여 잠재변수를 생성한 후 이 잠재변수를 이용하여 회귀모형을 적합하는 분석 방식이 부분최소제곱회귀이다.

PLS의 아이디어는, 원변수($X$)와 반응변수($y$)의 공분산을 최대로 하는 잠재변수를 만들어 회귀모형의 설명변수로 사용하는 것이다. 이를 수식으로 표현하면 다음과 같다.

```math
\boldsymbol {\hat y} = \sum_{i = 1}^k t_i b_i \tag {5.1}
```

이때 $t_i$는 $\boldsymbol X$의 선형변환($\boldsymbol X w$)을 의미하는데, 선형변환에 필요한 가중치 $w$를 구하는 과정에서 $\boldsymbol X$와 $\boldsymbol y$의 공분산이 최대가 되도록 조정하는 알고리즘을 거쳐 $t_i$의 최적값을 구한다. 이 알고리즘을 추상적으로 나타내면 아래와 같다.

```
(1) 각 변수 표준화

(2) X, Y의 공분산이 최대가 되도록하는 가중치 w를 계산하고 이를 이용하여 t를 계산

(3) t를 X, Y에 대해 각각 회귀모형 적합하여 계산한 잔차값을 새로운 X, Y로 갱신

(4) (2)~(3) 과정을 갱신하여 t값의 변화량이 충분히 작아지면 t를 확정

(5) (2)~(4)의 과정을 k번 반복
```

위 과정을 반복하여 완성된 회귀식 (5.1)은 ($n \times p$) 차원의 원변수 $\boldsymbol X$가 아니라 새롭게 생성한 k개의 잠재변수 $t_i$를 설명변수로, 원변수 $\boldsymbol Y$를 반응변수로 가진다. 이 회귀모형은 사전에 설정한 초모수(잠재변수의 개수 k, 알고리즘 종료의 기준이 되는 임계치)의 제약 하에서 원설명변수와 반응변수의 공분산을 최대한 설명할 수 있다. 또한 알고리즘 과정에서 주성분분석이나 인자분석과 비슷한 방식으로 $t_i$가 서로 직교하도록 조정되기 때문에 모형 내의 설명변수 간 독립도 보장된다.  

반면, 새롭게 생성한 변수 $t_i$는 $\boldsymbol X$의 선형결합으로 표현되기 때문에 잠재변수에 대한 해석의 문제가 PCA나 FA와 동일하게 여전히 존재한다. PCR의 경우 PC를 선정하는 과정에서 주관적인 의미나 변수의 개수를 선택한 후 회귀모형 적합이 이루어지지만 PLS의 경우에는 사전에 이러한 정보가 없는 상태에서 k를 정해야하는 어려움이 존재한다. 훈련용/검증용 데이터셋을 분리할 수 있는 경우 그리드 서치 방식으로 k를 1~p 사이에서 변화시켜가면서 과적합을 방지할 수 있는 범위 내에서 MSE가 최소가 되는 지점에서 k를 선택하기도 한다.

PLS는 예측에 초점을 둔 분석방법이기 때문에 PCR보다 이론적이고 직관적인 설명력은 떨어질 수 있다. 또한 예측값 최적화에만 목표를 둔다면 과적합이 일어나기 쉽다. 이런 점에서 PCR보다는 신경망 계열의 분석 방법에 근접한 분석 방식이라고 할 수 있다.


---

### 5.3. 다차원척도법

다차원척도법은 다차원에 있는 데이터를 저차원의 척도에 표현하는 데이터 분석 기법이다. (1) 데이터 간의 비유사성을 측정하는 값을 계산한 행렬을 만들고, (2) 비유사성 행렬값과 새로운 측도값을 입력값으로 가지는 목적함수(스트레스 함수)를 정의한 뒤, (3) 목적함수를 최소화하는 측도체계를 찾아내는 알고리즘을 반복하는 방식으로 원데이터의 비유사성을 최대한 보전하는 저차원의 측도체계를 찾아낸다.

#### 5.3.1. 비유사성

데이터의 비유사성을 정의하는 방법은 원데이터의 측도체계에 따라 달라진다. 비율척도, 구간척도를 따르는 경우에는 metric 비유사성을 이용하고 명목척도, 순서척도를 따르는 경우에는 non-metric 비유사성을 이용한다. metric 비유사성 중 대표적인 값은 유클리디안 거리로, i번째 관측치와 j번째 관측치의 유클리디안 거리는 아래와 같이 정의한다. (다양한 거리체계에 대하서는 lesson7에서 다룹니다.)

```math
\delta_{ij} = \| \boldsymbol x_i - \boldsymbol x_j \|_2 = \sqrt {\boldsymbol x_i - \boldsymbol x_j}
```

---

#### 5.3.2. 스트레스 함수

스트레스 함수는 (1) 데이터의 비유사성값과 (2) 새롭게 정의하려고 하는 측도체계값을 입력값으로 받는 함수로, 스트레스 함수가 작아진다는 의미는 새로운 측도체계가 원데이터의 비유사성을 **최대한** 보존한다는 의미가 된다. 가장 일반적으로 사용하는 크루스칼-쉐퍼드 스트레스 함수(raw stress 함수라고도 부릅니다.)는 아래와 같이 정의한다.

```math
S(\boldsymbol {z_1, z_2, \cdots, z_N} ) = \sum_{i \neq j} (d_{ij} - \| \boldsymbol z_i - \boldsymbol z_j \|)^2
```

$d_{ij}$는 원데이터에서 i번째 관측치와 j번째 관측치의 비유사성값이고, $z_i$는 i번째 관측치가 가진 값을 입력했을 때의 값을 벡터 형식으로 나타낸 값이다. ($\boldsymbol z_i \in \mathbb R^k$)

위 스트레스 함수를 해석하면 다음과 같다. $\mathbb R^k$ 차원에서 정의(k가 최종적으로 표현하고 싶은 차원체계)된 $\boldsymbol z$가 있을 때, i번째 관측치와 j번째 관측치에 대응하는 $z_i, z_j$값의 차이와 원데이터 관측치를 이용하여 계산한 거리값 $d_{ij}$와의 차이값이 스트레스 함수값이다. MDS에서는 일련의 알고리즘을 통해 스트레스 함수값이 작아지도록 $\boldsymbol z$의 체계를 조정하는 과정을 반복하여 최종적인 $\boldsymbol z$ 측도체계를 찾아낸다.

교재에 따라서는 다음과 같은 Sammon mapping 스트레스 함수를 제시하는 경우도 있다.

```math
S_{SM}(z_1, z_2, \cdots, z_N) = \sum_{i \neq j} \frac{(d_{ij} - \| z_i - z_j \|)^2} {d_{ij}}
```

raw stress와 비교했을 때에 Sammon mapping은 스트레스 함수를 통해 찾아내려고 하는 값이 절대적인 차이가 아니라 비율이 되기 때문에 스트레스 함수를 통해 원데이터의 비유사성을 새로운 측도체계로 변환하는 과정에서 손실되는 원데이터의 비유사성을 일정하게 유지하는 것을 목표로 한다. 비유사성 손실분의 비율에 맞춰 스트레스 함수를 정의하면 (1) 측정단위로 인한 비유사성값의 차이, (2) 비유사성값의 절대적인 차이로 인한 측도체계의 왜곡을 보정할 수 있다는 점에서 비유사성 손실에 대한 기준이 엄격해진다고 할 수 있다. (다만, 비유사성 행렬을 알고리즘에서 처리하는 과정에서 중심화 또는 표준화가 이루어지기 때문에 스트레스 함수값의 정의와 무관하게 (1)이 최종적인 측도체계에서 문제가 되는 경우는 없다고 생각해도 됩니다.)

---

#### 5.3.3. 측도체계 탐색

MDS는 유클리디안 거리를 $d_{ij}$로 사용하고 raw stress를 사용하는 경우 닫힌 해가 존재한다는 점이 증명되었으며, 이 경우 새로운 척도는 주성분 k개를 선택하는 것과 동치이다. 

raw stress 함수를 사용하더라도 유클리디안 거리를 이용하지 않거나 거리값에 가중치를 부여하는 경우에는 닫힌 해가 존재하지 않아 알고리즘에 따른 수치적 접근법에 따라 해를 구한다.

알고리즘의 순서는 아래와 같다.

(1) 거리 행렬의 이중중심화 행렬(평균을 0으로 맞추면서 행, 열, 전체 평균을 제거하는 과정을 거쳐 내적과 동치인 값을 구하는 과정을 거친 행렬을 뜻합니다. 중심으로부터 각 원소의 상대적인 위치를 보여줄 수 있기 때문에 알고리즘에서 사용할 $\boldsymbol z$의 초깃값으로 활용합니다.)을 구해서 $\boldsymbol z$의 초깃값으로 설정

(2) 초기의 $\boldsymbol z$와 거리행렬 간의 거리와 거리 차이의 비율을 반영하여 $\boldsymbol z$을 새롭게 계산

(3) $\Delta \boldsymbol z$의 크기가 임계치 아래로 내려갈 때까지 (2)를 반복

위 알고리즘은 SMACOF(Scaling by MAjorizing a COmplicated Function)으로 불리우며, 경사하강법과 같은 점진적인 조정방식과 비교했을 때에 최적값 계산이 빠르고 국지해(local minima)에 빠지는 위험이 적으며 최소값을 보장하는 것으로 알려져있다.

---

#### 5.3.4. $n \ll p$에 대한 활용성 평가

MDS는 원데이터의 p차원의 크기와 무관하게 데이터의 상대적인 비유사성을 최대한 보전할 수 있다는 점에서 $n \ll p$ 문제에 대한 해결책이 될 수 있다. 다른 차원축소 방법과 유사하게 원데이터의 정보를 일부 손실하는 대가로 단순화된 측도에 따라 데이터를 재배열할 수 있기 때문이다. 2차원 또는 3차원 공간에 각 관측치의 유사성을 기준으로 시각화할 수 있는 방법으로 MDS를 활용하는 경우가 많다.

PCA나 FA와 다른 점은, PCA와 FA가 변수를 기준으로 상대적인 공변량 구조를 차원축소하는 기법인 반면 MDS는 관측치 단위의 상대적인 변수값의 차이를 차원축소한다는 점에서 차이가 있다. 이 때문에 MDS를 이용한 분석결과는 변수 간의 상호관계를 해석하는데에 큰 도움이 되지 않는다. 따라서 변수의 관계를 파악하는 것이 목적이라면 MDS를 선택하는 것이 적합하지 않다.

반면, 변수간의 관계보다는 관측치 사이의 차별점이 분석의 초점이라면 MDS나 SOM과 같은 계열의 차원축소법을 활용하는 것이 좋다. 그래서 MDS를 활용한 연구분석은 유전자의 차이로 인한 개체의 표현형의 상이성, 설문조사를 기반으로 한 브랜드 간의 포지셔닝 차이 등 변수 자체에 관심이 없거나 변수의 관계보다는 개체의 차이점에 관심이 있는 경우에 제한된다. 

따라서, 연구분석의 목적에 따라 MDS 활용 여부를 잘 판단해야 한다.

---

### 5.4. 특잇값분해와 잠재의미분석(Latent Semantic Analysis, LSA)

$n \ll p$가 가장 심한 분야 중 하나는 자연어 처리이다. 자연어 처리의 단위가 형태소 분석에 기반한 어근이나 의미의 분절인 경우가 많기 때문에 분석의 대상이 되는 말뭉치(corpus)의 크기가 크면 클수록 처리의 단위가 커진다. 최근의 LLM 모형 역시 형태소 분석을 생략한 토큰을 입출력의 단위로 하고 있기 때문에 각각의 문서(자연어 분석에서는 관측치에 해당하는 데이터의 단위를 문서document라고 칭하는 경우가 많습니다.)는 다수의 토큰 중에서 일부를 선택한 형태로 정형화한다. 이 때문에 다수의 문서가 분석대상인 경우 데이터셋은 정형화 후 $n \ll p$의 형태가 되는 경우가 많다. 이러한 데이터셋을 확률론에 기반하여 분석하는 기법 중에서 잠재의미분석은 특잇값분해를 이론적 기반으로 한다.

4차시에서 살펴봤던 주성분분석은 고유값분해를 이론적 기반으로 하고 있는데, 고유값분해는 정방행렬만을 대상으로 하는 반면, 비정방행렬의 경우에도 고유값분해와 유사한 형태로 행렬의 선형사상변환 구조를 이해할 수 있는데 이러한 행렬의 분해 방법을 특잇값분해라고 한다.

특잇값분해를 통해 ($m \times n$) 차원의 행렬을 분해하면 아래와 같이 나타낼 수 있다.

```math
\boldsymbol A = \boldsymbol {U \Sigma V'}
```

이때, $\boldsymbol U$는 ($m \times m$) 행렬로 $\boldsymbol {AA'}$의 고유벡터로 이루어진 행렬이고, $\boldsymbol V$는 ($n \times n$) 행렬로 $\boldsymbol {A'A}$의 고유벡터로 이루어진 행렬, $\boldsymbol \Sigma$는 ($m \times n$) 행렬로, $\boldsymbol {U, V}$의 고유값의 제곱근을 대각원소로 하는 대각행렬이다.($\boldsymbol U$와 $\boldsymbol V$의 고유값은 모두 0 이상이며 0보다 큰 고유값을 공유합니다.) 고유값분해와 마찬가지로 특잇값분해 역시 행렬이 가지는 선형사상을 변환의 방향에 따라 보여준다.

이러한 특잇값분해의 성질을 이용하여 DTM(Document-Term Matrix, 문서 단어 행렬로 각 문서의 인덱스가 관측치 번호가 되고 corpus에 존재하는 모든 단어를 변수로 하는 정형화된 데이터셋.)을 특잇값분해하면 전체 문서의 상관관계를 가장 잘 표현해줄 수 있는 벡터를 찾아낼 수 있다. 이 벡터는 PCA나 FA와 동일하게 모든 단어 조합의 선형합의 형태로 표현되며, 다른 벡터와 비교해서 상대적으로 적재량에 큰 변화를 가지는 단어들이 해당 벡터의 대표단어가 된다. 이러한 방식으로 문서의 주제를 추출하는 분석 방법이 잠재의미분석이다.

잠재의미분석은 이론적인 근거를 바탕으로 빠르게 문서의 주제를 분리해낼 수 있다는 장점이 있다. 반면, 일회성 계산으로 토픽을 추출하기 때문에 새로운 자연어 데이터를 추가하거나 기존의 자연어 데이터와 성질이 다른 데이터에 대해서는 유효하지 않다는 단점이 있다. 이 때문에 LSA는 실전의 자연어 처리 프로젝트에서는 거의 사용되지 않고, 신경망에 기반한 자연어 처리의 원리를 이해하는 중간단계 정도로 활용된다.

---

### 5.5. 정준상관분석(Canonical Correlation Analysis, CC)

정준상관분석이란 두 그룹의 다변량 데이터 간의 상관관계를 찾아내기 위한 분석 방법이다. 두 그룹의 단변량 데이터는 단순 상관계수를 찾아내고, 둘 중 하나가 단변량 데이터인 경우에도 상관계수 벡터를 만들어낼 수 있지만, 두 그룹이 모두 다변량 데이터인 경우 두 집단의 연관성을 단순히 상관계수 행렬로 표현하는 데에는 한계가 있다. 이를 해결하기 위해 두 집단의 선형결합으로 구성된 새로운 **정준계수 벡터**가 두 벡터의 상관계수를 극대화하는 방식으로 생성하여 상관관계를 수식화할 수 있다. 

```math
\boldsymbol X \in \mathbb R^{(n \times p)}, \ \boldsymbol Y \in \mathbb R^{(m \times q)} \\

\boldsymbol U = \boldsymbol {a'X} \\

\boldsymbol V = \boldsymbol {b'Y} \\

\argmax_{U, V} Corr(U, V)
```

위 문제의 해는 $\boldsymbol X$와 $\boldsymbol Y$의 상관계수행렬을 고유값분해한 함으로써 구할 수 있다. 고유값분해의 특성상 두 집단 변수의 수가 각각 p, q개 있을 때에 고유값이 0보다 큰 고유벡터에 해당하는 정준계수(canonical weight / canonical coefficient)는 min(p, q)개가 나온다.

정준상관분석의 목적은 정준계수와 원데이터 변수값의 선형결합인 정준변수(canonical variate)를 찾아내는데에 있다. 잘 정리된 데이터라는 전제 하에 두 데이터셋을 기반으로 구한 정준변수는, 유사한 방법으로 측정한 다른 데이터셋의 상관관계를 가장 잘 보여줄 수 있는 변수의 축이 되기 때문이다. 

이 때문에 정준상관분석은 두 개의 다변량 데이터셋만을 분석해서는 의미있는 결과를 찾아내기가 힘들고, 두 개의 데이터셋을 통해 구한 정준변수를 적용할 수 있는 다른 데이터셋이 존재하는 경우에 활용성이 극대화된다. 그래서 심리학, 생물학, 교육학 등에서 심리검사나 적성검사의 설문척도 중에서 공통된 차원을 찾아내는데에 활용할 수 있다. 최근의 연구 중에서는 각 지번의 지리적인 특성 데이터와 침수 데이터를 CCA하여 침수 위험을 나타낼 수 있는 측도를 만들어내는 데에 활용한 [사례](https://scholar.kyobobook.co.kr/article/detail/4010070472322?utm_source=chatgpt.com)도 있다. 최근에는 신경망 모형에 CCA를 결합하여 멀티모달 AI의 개발에 활용하기도 한다.

---

### 5.6. 결론

PCA와 유사한 접근법을 통해 $n \ll p$인 경우 차원을 축소하여 분석의 목표에 접근해갈 수 있는 몇 가지 기법을 살펴봤다. 이들 기법은 공통적으로 (1) 회귀분석의 설명변수/반응변수 분리와 무관하고, (2) 수학적인 이론을 데이터 해석에 접목하여 다변량 데이터의 정보량을 일부 손실하고 차원을 축소, (3) 결과의 활용성에 극도로 전문적인 주관성이 필요하다는 공통점을 가지고 있다. 몇 차례 강조했던 것과 같이 $n \ll p$인 데이터는 데이터 자체가 가지고 있는 한계 때문에 일반적인 방법으로는 분석이 어려운 데이터이다. 따라서, 이러한 데이터를 극단적인 해석의 방식으로 다루는 방식이 lesson4와 lesson5에서 다루는 분석 방법이라고 이해할 수 있다. 

주의할 점은 각각의 분석방법이 매우 상이한 이론적인 기반과 전제를 가지고 있다는 점이다. 이들 분석방법을 활용하여 실제 분석을 진행하기 위해서는 이 점을 잘 이해하고 활용하는 것이 중요하다. (특히, MDS의 경우 본 강의의 흐름상 "차원축소"라는 점에 방점을 두고 분석기법을 소개했지만 많은 서적에서는 "지도학습을 통한 저차원 시각화"라는 점을 강조하며 별도의 분석기법으로 소개하는 경우가 많습니다. 어느 쪽에 중점을 두고 이해할지는 학습자의 편의에 달렸다고 생각합니다.) 그래서 항상 데이터 분석의 궁극적인 목표나 지향점이 무엇인지를 명확하게 세운 후에 데이터를 분석하는 접근법이 중요하다.