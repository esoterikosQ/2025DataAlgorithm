## 15. 의사결정나무

### 15.1. 개념

의사결정나무는 데이터 분석을 트리 구조로 수행하는 것이 핵심 내용입니다. 나무를 거꾸로 뒤집어 놓은 형태로 전체 데이터를 일정한 기준에 따라 이진 또는 다진 분류하는 것이 기본적인 원리인데, 사실 이 분류가 전부인, 매우 직관적이고 단순하면서도 실제 데이터 분석에서는 강력한 효과를 가지고 있는 분석 방법입니다.

의사결정나무에서는 각각 분기가 이루어지는 지점을 노드(node)라고 부릅니다. 전체 데이터를 두고 분류의 출발점이 되는 트리의 최상단 노드를 루트 노드(root node), 노드에서 분기가 이루어지기 전의 노드를 부모 노드(parent node), 분기가 이루어진 후의 노드를 자식 노드(child node)라고 부릅니다. 더 이상 자식 노드가 없는 노드를 리프 노드(leaf node)라고 부릅니다.

---

#### 15.1.1. 재귀적 분할(recursive partitioning)

의사결정나무는 데이터를 분할하는 것이 핵심입니다. 의사결정나무를 생성하는 과정에서 데이터를 분할하는 것을 일컬어 재귀적 분할(recursive partitioning)이라고도 부르는데요, 그 이유는 일반적인 알고리즘과 마찬가지로 데이터를 분할하는 과정이 모든 데이터 분할이 완료될 때까지 반복적(recursive)으로 이루어지기 때문입니다. 

일반적인 재귀적 분할 절차는 다음과 같습니다.

1. 전체 데이터셋에서 시작

2. 가장 좋은 분할 기준을 선택

3. 종료 조건을 만족하면 종료

4. 분할한 하위 노드에서 2~3번을 반복

이 과정을 통해서 전체 데이터셋에서 시작한 나무는 최종적인 하위 단계, 리프 노드에 도달하게 되는데, 리프 노드의 성질에 따라서 최종적으로 데이터의 성질을 정의하게 됩니다.

---

#### 15.1.2. 분할 과정의 수학적 이해

의사결정나무에서 데이터의 분할은 "나무의 성장"이라고 비유적으로 설명합니다. 즉, 데이터의 분할이 반복될수록 나무가 자라는 겁니다. 최종적으로 생성한 분류의 기준이 나무를 거꾸로 뒤집어놓은 형상과 비슷하기 때문에 분석 기법의 이름부터가 "나무"라고 명명되었는데, 데이터 분할은 어떤 기준으로 수행하는 것인지에 대해 살펴보겠습니다.

나무를 분할하는 기준을 한 마디로 표현하면 **"분할하는 것이 분할하기 전보다 동질성이 크게"**라고 할 수 있다. 이때에 **"동질성"**을 정의하고 측정할 수 있는 방법이 여러 가지가 있습니다. 일반적으로 의사결정나무를 설명할 때에 이러한 기준을 **불순도(impurity)**라고 표현합니다.

불순도는 집합 내에서 서로 다른 클래스의 데이터가 섞여 있는 정도를 수치화한 것입니다. 불순도가 낮을수록(0에 가까울수록) 집합이 순수하며, 높을수록 여러 클래스가 혼재되어 있다는 의미입니다. 의사결정나무는 이 불순도를 최소화하는 방향으로 분할을 진행합니다.

---

1. 엔트로피(Entropy)

엔트로피란 Claude Shannon이 1948년 정보 이론(Information Theory)에서 제안한 개념으로, 어떤 상태의 불확실성 또는 무질서를 측정하는 지표입니다. 데이터 분류의 문제에서 엔트로피는 한 군집에 속해있는 데이터 관측치 간의 **예측 불가능성**으로 이해할 수 있습니다. 즉, 동일한 군집에 여러 개의 관측 데이터가 속해있는데 이들 데이터 중에서 어느 한 부류에 속할 가능성이 월등하게 높으면 엔트로피값은 낮아집니다. 이를 직관적으로 해석하면 "부모 노드에서 자식 노드로 분할되는 과정을 겪었더니 자식 노드 내에서 데이터들이 매우 동질적이더라"로 이해할 수 있습니다. 

엔트로피를 수식으로 정의하면 다음과 같습니다.

```math
H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)
```

여기서 $S$는 데이터의 집합, $c$는 각각의 클래스(부류), $p_i$는 클래스 $i$에 속하는 샘플의 비율을 의미합니다. 

엔트로피는 (0, $\log_2(c)$) 구간의 값을 가지며, 값이 0에 가까울수록 집합 $S$ 내의 데이터들이 동일한 클래스에 속해있다는 의미입니다. 반대로 값이 $\log_2(c)$에 가까울수록 집합 $S$ 내의 데이터들이 여러 클래스에 고르게 분포되어 있다는 의미입니다. 또한, 로그함수의 일반적인 형태와 동일하게 위로 볼록한 형태의 함수를 가지며, 함수값은 균등분포일 때에 최대값을 가집니다.

위 식으로 표현한 엔트로피는 집합 $S$에 해당하는 전체 관측치 모든 관측치가 어느 클래스에 해당하는지를 완전하게 판단하기 위해서 필요한 평균 정보량의 의미를 가집니다. 여기서 **정보량**이란 이진분류의 방식으로 전체 데이터의 관측치들에게 모두 클래스를 부여하기 위해서 필요한 분류의 횟수, 즉, 분류를 위한 기준의 갯수를 의미합니다. 극단적으로 모든 관측 데이터가 동일한 클래스에 속한다면 분류가 필요없어집니다. $\log_2(1) = 0$으로 표현할 수 있습니다. 반대로 관측 데이터가 많은 수의 클래스에 해당하고 각각의 클래스에 속하는 비율이 균등해질수록 필요한 분류의 횟수가 많아집니다. 극단적으로 10개의 관측치가 모두 다른 클래스에 속한다면 9번의 분류가 필요해지고, 클래스의 숫자가 많아지면 많아질수록 분류에 필요한 횟수도 늘어납니다. 따라서, 엔트로피가 가장 많이 낮아진다는 의미는 부모-자식 노드 이동을 통해 추가적으로 필요한 분류의 횟수가 가장 많이 줄어든다, 즉, 완전 분류를 위해 필요한 정보량이 줄어든다는 의미로 해석할 수 있습니다.

---

2. 지니 불순도(Gini impurity)

지니 불순도는 소득의 불평등 정도를 측정하기 위해 사용하는 경제학의 지니계수(Gini coefficient)에서 유래한 개념입니다. 경제학에서의 지니계수는 소득분배가 완전히 평등한 상태에서의 누적소득분포도 대비 현재의 누적소득분포도의 면적의 비율을 기준으로 측정합니다. 이때에 사용하는 수식이 지니 불순도와 정확하게 일치합니다. 아래와 같이 표현할 수 있습니다.

```math
Gini(S) = 1 - \sum_{i=1}^{c} p_i^2
```

지니 불순도는 (0, $1 - \frac 1 c$) 사이의 값을 가집니다. 0에 가까울수록 불순도가 낮은, 집합 내 관측치의 동질성이 높고, $1 - \frac 1 c$에 가까울 수록 불순도가 높아집니다. 로그값을 계산하는 과정이 없기 때문에 엔트로피보다 연산량이 적으면서도 확률의 제곱의 형태를 취하기 때문에 볼록함수의 형태를 공유한다는 점도 장점이라고 할 수 있습니다. 불순도의 최대값은 역시 균등분포의 경우 최대값을 가집니다.

지니 불순도는 집합 S에서 무작위로 샘플을 선택하고 원래 집합의 클래스 별 비율에 따라 무작위로 클래스를 부여했을 때에 원래 레이블과 다를 확률로 해석할 수 있습니다. 이는 수식을 통해서도 확인할 수 있습니다. 

```math
\sum_{i=1}^{c} p_i \sum_{j \neq i} p_j = \sum_{i=1}^{c} p_i (1 - p_i) = \sum_{i=1}^{c} p_i - \sum_{i=1}^{c} p_i^2 = 1 - \sum_{i=1}^{c} p_i^2
```

엔트로피와 비교했을 때에, 지니 불순도는 불순도 증가에 덜 민감하고 계산이 빠르다는 단점이 있습니다만, 실제로 의사결정나무를 적합했을 때에 두 지표가 가지는 차이는 미미한 것으로 알려져있습니다.

---

3. 정보 이득(Information Gain)

정보 이득이란 개념은 의사결정나무를 적합하는 기법 중 ID3 알고리즘을 고안하는 과정에서 도입한 개념입니다. 본질적으로는 엔트로피 감소량과 동일한 의미입니다.

```math
IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)
```

여기서, $A$는 분할의 기준이 되는 속성을, $Values(A)$는 속성 $A$가 가질 수 있는 값들의 집합을 의미하고, $S_v$는 속성 $A$의 값이 $v$인 부분집합을 의미합니다. $|S|$ 기호는 집합 $S$ 내에 포함된 관측치의 숫자를 의미합니다. 위 수식을 통해서, 개념상 정보이득을 측정하기 위해서는 두 종류의 속성이 필요하다는 점을 알 수 있습니다. 즉, 측정의 기준이 되는 집합 $S$와 분할의 기준이 되는 속성 $A$입니다.

정보 이득은 (0, H(S))의 구간을 가지며, 특정 속성을 기준으로는 어떤 방법으로 분할해도 불확실성이 감소하지 않는 경우(무작위로 배정되는 속성) 0의 값을 가지고, 분할의 기준이 되는 속성이 클래스를 완전히 결정할 수 있는 경우(각각의 관측치를 고유하게 구분할 수 있는 속성)에 H(S)의 값을 가집니다.

정보 이득을 수학적으로 해석해보면 두 부분으로 나눌 수 있습니다. 초항인 $H(S)$는 분할 전의 불확실성(엔트로피 값)을 의미하고, 두 번째 항인 $\sum_{v} \frac{|S_v|}{|S|} H(S_v)$는 분할 후의 가중 불확실성을 의미합니다. 여기서 가중의 기준은 분류 전 대비 분류 후 집합의 크기가 됩니다. 즉, 분할 후에 많은 관측치를 포함하는 집합에 더 많은 가중치를 부여하여 각각 분할된 데이터들이 가지는 엔트로피값의 합입니다. 따라서, 정보 이득은 불확실성의 감소량을 $IG(S, A) = \text{Before} - \text{After}$의 형태로 표현한 것으로 이해할 수 있습니다. 

한편, 엔트로피가 아니라 지니 불순도를 사용하여 분할 전후의 불순도 변화를 비교하는 **지니 이득(Gini gain)**의 개념도 존재합니다. 정보 이득과 비슷하게 아래와 같이 정의할 수 있습니다.

```math
GiniGain(S, A) = Gini(S) - \sum_{v} \frac{|S_v|}{|S|} Gini(S_v)
```

정보 이득이 가지고 있는 단점은 클래스가 많은 속성을 선호하는 편향이 있습니다. 즉, 주민번호와 비슷하게 각각의 관측치에게 주어진 고유한 값이 관측치의 수만큼 존재하는 경우에 정보 이득이 최대가 되는 문제가 있습니다. 이 때문에 정보 이득을 의사결정나무 분할의 기준으로 사용하는 경우 과적합이 발생할 가능성이 높습니다. 이를 보완하기 위해 **이득 비율(Gain ratio)**이라는 개념이 등장합니다. 수식으로는 아래와 같이 표현합니다.

```math
GainRatio(S, A) = \frac{IG(S, A)}{SplitInfo(S, A)}
```

여기서 분모에 해당하는 값은 분할 정보(split information)라는 개념으로 다음과 같이 표시할 수 있습니다.

```math
SplitInfo(S, A) = -\sum_{v} \frac{|S_v|}{|S|} \log_2 \frac{|S_v|}{|S|}
```

위 수식은 속성 $A$ 자체의 엔트로피로, 값이 많을수록 커지기 때문에 이 값을 기준으로 정보 이득을 나눠주면 정규화하는 효과가 발생합니다. 

---

#### 15.1.3. 종료 조건과 가지치기

의사결정나무는 분할을 반복적으로 수행하면서 성장하는데, 이 성장이 종료되는 조건이 알고리즘에 내장되어 있거나(사전 가지치기), 성장이 끝난 후 가지치기(사후 가지치기)를 수행하는 방식으로 과적합을 방지합니다.

성장이 종료되는 조건은 노드의 모든 샘플이 리프 노드에서 같은 클래스에 속하거나, 더 이상 분할할 수 있는 속성이 없거나, 사전에 설정한 최대 깊이 또는 노드 내 관측치의 최소 수 미만으로 분리가 완료되는 등으로 설정합니다. 

사후 가지치기는 나무를 완성한 후 검증이나 평가를 통해 불필요한 가지를 제거하는 방법을 사용합니다.

---

### 15.2. 나무 생성 알고리즘의 종류

#### 15.2.1. CHAID (Chi-square Automatic Interaction Detection)

Kass가 1980년 개발한 CHAID 알고리즘은 의사결정나무 생성 알고리즘의 초기 형태 중 하나입니다. CHAID는 매 분기마다 카이제곱 검정(Chi-square test)을 사용하여 통계적으로 유의미한 분할을 찾아내는 기법입니다. 카이제곱 통계량은 아래와 같이 정의됩니다.

```math
\chi^2 = \sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
```

여기서, $O_{ij}$는 관찰된 빈도값, $E_{ij}$은 분포가 균등하다고 가정했을 때에 기대할 수 있는 빈도값($\frac{(행 합계) \times (열 합계)}{전체 합계}$)을 의미합니다.

알고리즘의 전개 과정은 아래와 같습니다. 

```
1. 각각의 예측 변수에 대해 카이제곱 검정 수행
    1-1. 모든 클래스가 분리된 상태에서
    1-2. 인접한 클래스 쌍에 대해 카이제곱 검정 수행
    1-3. p-value가 사전 정의된 임계값보다 큰 경우 클래스 병합
    1-4. 병합이 불가능한 지점에서 분할을 종료
2. 합병된 각 변수의 범주들과 목적 변수에 대해 카이제곱 검정을 수행하여 다중검정 보정을 적용한 결과 가장 작은 p-value를 가진 변수 기준으로 최적 분할 선택
3. 종료 조건이 충족되면 종료, 아니면 각각의 자식노드에 대해 1~2 과정을 수행
```

CHAID는 통계적 유의성이 기반하여 분할지점을 선택하기 때문에 엄밀한 이론적인 기반을 가지고 있습니다. 또한 인위적으로 각각의 범주에 해당하는 관측치는 하나의 묶음으로 묶어 분할하기 때문에 노드의 결과를 해석하기가 쉽다는 장점이 있습니다. 또한, 알고리즘의 특성상 다지분할이 가능합니다. 반면, 카이제곱 검정의 특성상 범주형 변수에만 적용 가능하기 때문에 연속형 변수의 경우 범주화를 선행해야 하고, 각각의 분기마다 검정을 수행해야하기 때문에 계산 비용이 높다는 단점이 있습니다.

---

#### 15.2.2. CART (Classification and Regression Trees)

Breiman 등이 1984년 고안한 기법으로, 이름에서 나타나는 바와 같이 범주형 변수만 분류할 수 있던 CHAID의 제약을 극복하고 회귀 문제에도 적용할 수 있는 알고리즘입니다. 분류의 경우에는 지니 불순도를, 회귀의 경우 분산을 사용하여 분할을 수행합니다. 

알고리즘의 전개 과정은 다음과 같습니다.

```
1. 루트 노드에서 시작
2. 모든 가능한 이진 분할 탐색(각각의 속성별로 모든 관측치를 분할하는 조합의 수를 탐색)
3. 지니 불순도(또는 분산)를 최소화하는 분할 선택
4. 재귀적으로 하위 노드 생성
5. 종료 조건 만족 시 멈춤, 아니면 2~4 단계 반복
```

CART 알고리즘을 이용하는 경우 비용-복잡도 가지치기를 병행할 수 있습니다. 가지치기의 기준이 되는 비용함수는 다음과 같이 정의할 수 있습니다.


```math
R_\alpha(T) = R(T) + \alpha |T|
```

여기서, $R(T)$는 트리 $T$의 재대입 오차(resubstitution error)로 트리를 적합하는 데에 사용했던 데이털르 그대로 사용했을 때에 최종적인 리프 노드에서 분류가 잘못되는 비율을 의미합니다. $|T|$는 리프 노드의 개수를 의미하고, $\alpha$는 복잡도 패널티 파라미터로, 값이 커질수록 가지치기가 쉽게 일어납니다. 수행 가능한 가지치기의 모든 경우에 대해 비용함수를 계산한 후 최종적인 트리를 선택하는 과정입니다.

CART 알고리즘은 CHAID와 비교했을 때, 연속형 변수를 처리하는 회귀문제 해결이 가능하다는 장점을 가집니다. 또한 체계적인 가지치기 방법을 제공한다는 점도 특징입니다. 단점으로는 분산을 불순도의 기준으로 사용하는 회귀문제에서 이상치의 존재에 영향을 크게 받는다는 점이 있습니다. 또한, 회귀분제도 나무 모형을 이용하여 접근이 가능하지만 선형회귀와 다르게 특정한 값에서 분할을 시도하는 지점만을 제시한다는 점에서 선형 관계를 포착하기 어렵고 부모 노드를 분할한 기준과 자식 노드를 분할하는 기준이 상충되는 경우 해석이 어려울 수 있다는 단점이 있습니다.

---

#### 15.2.3. ID3 (Iterative Dichotomiser 3)

ID3는 Ross Quinlan이 정보 이론에 기반하여 고안하여 1986년 제안한 나무 생성 알고리즘입니다. 정보 이득을 기준으로 분할을 수행하는 것이 핵심입니다. 

구체적인 알고리즘은 다음과 같습니다.

```
1. 루트 노드에 전체 데이터 할당한 상태에서 
2. 각각의 속성에 대해 자식 노드를 분할했을 때의 정보 이득을 계산하고, 이 중 정보 이득이 최대인 속성을 선택
3. 선택된 속성값을 기준으로 분기 생성
4. 종료 조건(더 이상 사용할 속성이 남아있지 않거나 리프 노드에서 모든 관측치가 순수하게 분류된 경우)을 만족하는 경우 성장을 중단하고, 그렇지 않으면 2~3단계 반복
```

ID3는 CART와 비교했을 때에 직관적인 해석이 가능한 분기를 생성할 수 있다는 장점이 있습니다. 즉, 전체 관측치를 조합하여 이진 분류를 만들어내는 CART의 특성상 노드에서 분기는 불완전하게 발생할 수도 있습니다. 반면, ID3는 분기의 기준이 되는 속성을 선택하면 해당 속성에 포함된 클래스의 수만큼 분기가 발생합니다. 따라서, "날씨가 맑으면 1번 자식노드, 흐리면 2번 자식노드, 비가 오면 3번 자식노드"와 같은 방식으로 해석이 매우 쉬운 나무를 생성합니다.

반면, 연속형 변수의 경우 전처리로 범주화를 해줘야하는 단점을 가지고 있고, 정보 이득의 특성 때문에 클래스의 수가 많은 속성을 선호하는 편향이 있습니다. 또한 가지치기 기능이 없기 때문에 과적합될 위험이 존재합니다.

---

#### 15.2.4. C4.5

C4.5는 Ross Quinlan이 ID3의 주요 단점들을 개선한 알고리즘으로 1993년 제안했습니다. 

우선 정보 이득이라는 기준이 다중 클래스를 선호하는 문제를 해결하기 위해 이득 비율을 기준으로 분기를 수행합니다. 또, 연속형 변수를 처리할 수 있는 알고리즘을 내장했는데, 연속형 변수에 해당하는 관측치를 오름차순으로 정렬하고, 각각의 값 사이의 중간점을 임계값 후보로 설정한 후 각각의 임계값으로 분할했을 때의 이득 비율을 계산하여 이득 비율이 가장 커지는 점을 분할 지점으로 선택하는 방식입니다. 이 기준에 따라 연속형 변수의 경우 다진분할이 아니라 이진분할을 수행합니다.

기존의 의사결정나무에서는 결측값을 처리할 수 있는 기준이 없었는데, C4.5에서는 결측값을 배분하는 확률값에 기반하여 결측값을 포함하는 관측치를 분기하는 메커니즘을 도입했습니다. 마지막으로 오류 기반 가지치기(error-based pruning)라는 방식을 도입했습니다. 리프 노드를 자식 노드로 두고 있는 부모 노드에서 분기가 없는 경우와 분기가 있는 경우를 비교하여 분기가 있는 경우의 오분류율이 커지면 가지치기를 수행하는 방식입니다.

그 이외의 알고리즘 전개 과정은 ID3와 동일합니다.

C4.5는 ID3의 단점을 효과적으로 보완했으며 연속형 변수를 처리할 수 있는 기준을 내장함으로써 전처리 없이도 모든 유형의 데이터에 사용할 수 있는 범용성을 확보하고 있습니다. 또한 연속형 변수를 분산과 같이 이상치의 영향을 받을 수 있는 기준이 아니라 변수 사이의 중간값을 기준으로 분할하여 강건한(robust) 분할을 수행한다는 점도 장점입니다. 

단점으로는 여전히 분류 문제에만 적용이 가능하다는 특징이 있습니다. 즉, 관심의 대상이 되는 타겟 변수는 범주형 변수여야 합니다. 또, 정보 이득보다 계산 비용이 높아 대용량 데이터를 적합할 때에 연산 시간이 오래 걸릴 수 있다는 점도 단점입니다. 

---

### 15.3. 의사결정나무의 불확실성

15.2에서 서술한 나무 생성 알고리즘은 확정적인 데이터를 명확한 기준으로 선정하여 의사결정나무를 성장시키기 때문에 동일한 데이터에 대해서 동일한 기준으로 나무를 생성하는 경우 항상 동일한 나무가 생성됩니다. 동일성을 훼손할 수 있는 경우는 동점 상황에서 어떤 변수를 우선적으로 선택하느냐의 문제인데, 이 경우에도 난수 발생 순서를 고정시킨다든지 변수 입력 순서를 고정하는 방법으로 동일한 결과를 얻을 수 있습니다.

그런데, 실제 알고리즘을 구현한 소프트웨어 패키지에서는 동일한 데이터에 대해 매번 다른 나무가 생성되는 경우가 종종 발생합니다. 이는 알고리즘 구현 과정에서 의도적으로 무작위성을 도입했기 때문입니다. 분할의 기준이 되는 속성의 수를 무작위로 선택한다든지, 분할 기준이 되는 속성을 선택할 때에 의도적으로 속성 검토의 순서를 무작위로 섞는 등의 기법이 사용됩니다. 

이러한 무작위성의 도입은 단순한 구조의 의사결정나무가 가지는 결정론적인 성격 때문에 발생 가능성이 높은 과적합을 방지하기 위해 의도적으로 도입된 장치입니다. 의사결정나무를 생성하는 과정에서는 매 분기마다 모든 발생 가능한 분기의 상황을 검토하여 최적 분기를 결정하는데, 이 과정에서 데이터에 내재된 노이즈에 매우 민감하게 반응하는 특성이 있습니다. 이 때문에 하나의 데이터셋이 정해지면 그 데이터셋에 최적화된 나무가 생성되기 쉽고, 이는 의사결정나무를 일반화하여 사용하기 어렵게 만드는 장애물로 작용합니다. 이러한 의사결정나무의 한계를 보완하기 위해 의도적으로 무작위성을 도입하여 의사결정나무를 생성합니다.

---

### 15.4. 랜덤 포레스트(Random Forest)

#### 15.4.1. 앙상블 학습의 개념

의사결정나무는 과적합 가능성이 매우 높은 데이터 분석 기법입니다. 이를 극복하기 위해 여러 개의 의사결정나무를 결합하여 사용하는 방법이 고안되었는데, 이를 랜덤 포레스트 기법이라고 부릅니다. 이 기법은 Leo Breiman이 2001년 제안한 방법으로, 여러 개의 의사결정나무를 무작위로 생성한 후 각각의 나무가 예측한 결과를 결합하여 최종적인 예측을 수행하는 방법입니다.

랜덤 포레스트의 기본 원리인 "여러 개의 모델을 결합하여 단일 모델보다 성능이 더 나은 모델을 얻는 방식"을 **앙상블 학습(ensemble learning)**이라고 합니다. 직관적으로는 집단지성을 통한 최적의 답안 도출로 이해할 수 있는 앙상블 학습의 작동 원리는, 모델들이 서로 다른 오류를 만들어야 하고(다양성), 각각의 모델이 랜덤으로 추측한 것보다는 나은 성능을 가져야(정확성)합니다. 

그런데, 최초에 제안한 방식으로 동일한 데이터에 대해 의사결정나무를 적합하는 경우 다양성 조건이 충족되지 않는 문제가 발생합니다. 이를 극복하기 위해 Breiman은 부트스트랩 샘플링이라는 기법을 도입했습니다. 

---

#### 15.4.2. 배깅(Bagging)

배깅은 Bootstrap Aggregating의 약자로, 다수의 의사결정나무를 생성하는 과정에서 원본 데이터를 그대로 이용하지 않고 부트스트랩 샘플링을 거친 데이터를 이용하여 각각의 나무를 생성하는 기법입니다. 배깅에서 부트스트랩은 복원 추출 방식으로 샘플링을 수행합니다. 이론상 n개의 데이터를 생성했을 때에 특정 관측치가 포함되지 않을 가능성은 다음과 같습니다. 

```math
P(샘플\ i가\ 선택\ 안됨) = \left(1 - \frac{1}{n}\right)^n \approx e^{-1} \approx 0.368
```

따라서, 부트스트랩을 거쳐 선택한 샘플은 약 63.2%가 중복없는 관측치이고, 나머지 36.8%는 중복된 관측치를 가집니다. 이러한 부트스트랩 샘플링을 통해 각각의 모델이 서로 다른 데이터를 학습하게 되어 다양성 조건을 충족시킬 수 있습니다.

부트스트랩을 거친 후 여러 개의 의사결정나무를 생성한 후, 각각의 나무에서 예측한 결과를 합산할 때에 분류 문제는 다수결을, 회귀 문제는 평균을 기반으로 최종적인 의사결정을 하게 됩니다. 

배깅은 분산을 감소시키기 때문에 예측의 정확도가 높아지는 것으로 알려져 있습니다. 예측의 잔차는 편향, 분산, 예측불가능한 오차로 이루어지는데, 다수의 모델을 합산하면 분산이 감소하는 효과가 있습니다. 

---

#### 15.4.3. 랜덤 포레스트의 원리

Breiman은 배깅을 개량한 모형으로 랜덤 포레스트를 2001년 제안했습니다. 배깅의 핵심은 의사결정나무가 가지는 과적합의 단점을 보완하기 위해 부트스트랩을 이용하여 무작위성을 추가하는 것이었는데, 실제로 데이터를 적합했을 때에 부트스트랩 샘플링을 거친 데이터를 사용하더라도 트리들의 구조가 크게 달라지지 않는 문제가 있었습니다. 즉, 여러 개의 속성 중에서 예측에 결정적인 영향을 미치는 속성이 존재하는 경우, 샘플링 과정을 거치더라도 해당 속성이 가지고 있는 영향력이 모든 트리에 반영되기 때문에 앙상블 학습이 실효성을 가지기 위한 조건 중 하나인 다양성이 충족되지 않는 문제가 발생한 것입니다.

랜덤 포레스트는 이러한 문제를 해결하기 위해서 추가로 속성을 무작위로 선택하는 과정을 도입했습니다. 의사결정나무를 생성하는 단계에서 각각의 분기를 결정하는 속성을 무작위로 선택하는 과정을 도입하여 각각의 나무들이 가지는 상관관계를 극적으로 감소시킨 것입니다. 

랜덤 포레스트의 알고리즘은 다음과 같습니다.

```
1. $B$개의 부트스트랩 샘플 생성
2. 각 샘플에 대해 결정 트리 학습:
   - 각 노드에서 분할 시:
     - 전체 p개 특성 중 무작위로 m개 선택 (보통 m << p)
     - 선택된 m개 중에서만 최적 분할 찾기
   - 가지치기 없이 최대한 성장 (각 트리는 높은 분산, 낮은 편향)
3. 모든 트리의 예측을 결합 (투표 또는 평균)
```

일반적으로 $m = \sqrt{p}$ 또는 $m = \log_2(p)$을 기준으로 선택합니다.

---

#### 15.4.4. 성능 평가

1. Out-of-Bag (OOB) 오차

부트스트랩을 거치면 모든 트리는 원본 데이터 중에서 평균적으로 37%의 샘플을 사용하지 않게 됩니다. 이를 OOB 샘플이라고 부릅니다. 랜덤 포레스트의 성능을 평가하는 기준 중 하나로 이 OOB 샘플을 이용한 검증 오차, 즉 OOB 오차를 사용합니다. OOB 오차는 별도의 검증 데이터 없이 기존의 데이터만으로 성능을 추정할 수 있다는 장점을 가지고 있습니다. OOB 오차의 정의는 아래와 같은 수식으로 표현할 수 있습니다.

```math
OOB\ Error = \frac{1}{n}\sum_{i=1}^{n} I(y_i \neq \hat{y}_i^{OOB})
```


2. 속성 중요도(feature importance)

속성 중요도란, 의사결정나무의 리프 노드에서 관심 변수의 값을 예측할 때에 어떤 속성이 얼마나 기여하는지를 나타내는 지표입니다. 최종적인 예측에 일관성있게 영향을 미치는 속성은 속성 중요도가 높게 나타납니다.

속성 중요도를 계산하는 방법은 두 가지가 있습니다. 하나는 평균 불순도 감소를 기준으로 측정하는 방법인데, 아래와 같은 식으로 계산합니다.

```math
Importance(X_j) = \frac{1}{B}\sum_{b=1}^{B}\sum_{t \in T_b: v(t)=X_j} \Delta i(t)
```

위 식을 해석하면, 포레스트 내의 모든 트리 $b = 1, 2, ..., B$에 대해, 각각의 트리 $T_b$에서 자식 노드 분기에 속성 $X_j$를 사용한 모든 노드 $t$에 대해 불순도 감소분 $\Delta i(t)$를 합산한 후, 각각 트리의 감소분 합산치를 평균내는 방식입니다. 

평균 정확도 감소를 기준으로 속성 중요도를 측정하는 방법은, 의사결정나무 적합에 사용하지 않은 OOB 데이터에서 특정 속성 $X_j$의 값만을 무작위로 섞어서 측정한 OOB 오차가 원래의 OOB 오차와 얼마나 차이가 있는지를 측정하는 방식입니다. 이 기준으로는 평균 정확도 감소값이 클수록 예측에 중요한 역할을 하는 속성이라고 해석할 수 있습니다.

---

#### 15.4.5. 장단점

랜덤 포레스트는 의사결정나무가 가지는 과적합 성향을 효과적으로 제어하여 높은 예측 성능을 제공하는 분석 기법입니다. 최종적인 예측 성능이 뛰어날 뿐만 아니라 속성 중요도, OOB 오차 등 객관적으로 데이터의 특성이나 모델의 성능을 측정할 수 있는 지표를 제공한다는 점도 강점입니다. 또한, 결측값을 포함하는 데이터 역시 처리할 수 있고, 병렬 처리가 가능하다는 점에서 대용량 데이터에도 효율적으로 적합할 수 있습니다.

반면, 여러 개의 트리가 결합된 형태이기 때문에 각각의 트리가 가지는 해석이 달랐을 때에 어떻게 이를 종합할 것인지에 대한 해석 여지가 높지 않다는 단점이 있습니다. 즉, 다수결 또는 평균의 방법으로 최종적인 결론은 도출하지만 어째서 최종적인 결론이 도출되었는지를 설명할 수 없습니다. 또한 많은 수의 의사결정나무를 생성하기 때문에 메모리 사용량이 많고 하드웨어의 성능이나 코딩 조건에 따라 모델의 성능, 분석 시간이 천차만별이라는 단점도 존재합니다. 

---

### 15.5. 부스팅(Boosting)

#### 15.5.1. 원리와 발전 과정

부스팅은 학습기(learner)라는 개념을 도입하여 다수의 약한 학습기를 순차적으로 학습시켜 최종적으로 강력한 성능을 내는 하나의 거대한 모형을 만드는 앙상블 기법입니다. 다수의 의사결정나무가 병렬적으로 의사결정을 한 후 이를 종합하는 배깅이나 랜덤 포레스트와 가장 큰 차이점은 학습기들이 연쇄적으로 작동한다는 점입니다. 즉, 앞선 학습기에서 제대로 예측하지 못한 샘플에 더 높은 가중치를 두어 다음 학습기는 이를 보완하도록 학습을 시킵니다. 

최초의 부스팅 알고리즘은 1990년 Freund가 제안한 AdaBoost(Adaptive Boosting)입니다. 여기서 제시한 "학습기"의 조건은 랜덤 추측보다 약간만 나으면 된다는 매우 낮은 기준이었고, 실제로 AdaBoost에서 사용한 학습기는 Decision Stump라는 이름의 1층짜리 의사결정나무였습니다. Yes/No로 구분되는 문제를 단 하나의 속성만을 이용하여 예측하는 매우 간단한 모델이었는데, 학습기를 거쳐 나온 결과값을 기준으로 틀린 샘플에는 더 높은 가중치가 부여되는 알고리즘을 고안하였습니다. 최종적으로는 훈련 오차가 0에 수렴한다는 이론적인 보장도 함께 제시했습니다. 

이후 Jerome Friedman은 1999년 회귀 트리를 기반으로 하는 Gradient Boosting 모형을 고안했는데, 회귀 트리를 거쳐 계산한 잔차를 기준으로 회귀 트리를 다시 적합하는 것을 반복하는 방식으로 전체 모형의 정확도를 개선하는 방법입니다. 

---

#### 15.5.2. 다양한 부스팅 모형

1. XGBoost

XGBoost는 Chen이 2016년 발표한 모형으로 머신러닝 경진대회에서 가장 많이 활용하는 분석 모형입니다. Gradient Boosting과 비교했을 때에 목적함수를 정규화하여 과적합을 방지하였고, 손실함수를 계산할 때에 테일러 근사를 이용하여 계산 속도를 높였습니다. 또, 트리 기반의 학습기를 사용하는데, 트리를 분할하는 과정에서 이득을 기반으로 분할 탐색하는 알고리즘을 도입하여 속도와 성능을 동시에 개선한 것이 특징입니다. 또한, 트리를 생성하는 과정에서 속성과 그 값을 병렬처리하는 알고리즘을 도입하여 학습기를 순차 사용하는 분석 방법이면서도 병렬처리의 장점을 살려 효율성을 높였습니다.

XGBoost는 (1) 약하지만 빠른 학습기를 이용하고, (2) 학습기를 연쇄적으로 적합할 때에 사용하는 기준값을 정교화하는 동시에 근사치를 기준으로 계산하여 계산 속도를 향상시켰고, (3) 의사결정나무를 적합하는 과정에서 이득을 기준으로 계산하되 탐색하는 과정을 최적화한 모형이라고 이해할 수 있습니다. 

2. LightGBM (Microsoft, 2017)

LightGBM은 대용량 데이터 처리에 특화하기 위해 고안한 부스팅 알고리즘입니다. 기존의 의사결정나무는 적합 과정에서 "깊이 우선" 전략을 사용하는데, LightGBM은 "잎사귀 우선(Leaf-wise)" 전략을 사용하여 손실 감소가 가장 큰 잎사귀 노드를 우선적으로 분할하는 방식을 사용합니다. 이를 통해서 동일한 수의 노드를 사용하는 경우에도 더 낮은 손실을 달성할 수 있습니다. 또, 앞선 예측기에서 예측에 실패한 관측치는 모두 유지하고 예측에 성공한 관측치는 일부만 샘플링하는 방식으로 계산량을 줄이는 GOSS 기법을 도입했습니다. LightGBM은 XGBoost와 비교했을 때에 속도가 더 빠르고 메모리 사용량이 적어 사용하는 데이터가 대용량일 때에는 LightGBM이 더 유리한 것으로 알려져 있습니다. 동일한 환경에서는 XGBoost가 성능 측면에서는 더 우수한 것으로 알려져 있습니다.

3. CatBoost (Yandex, 2017)

CatBoost는 러시아의 IT 기업인 Yandex에서 개발한 부스팅 알고리즘입니다. 기존의 부스팅 알고리즘에 존재하는 타겟 속성의 누수 문제를 해결하기 위해 클래스에 순서를 부여하여 앞선 관측치의 타겟 속성값만을 사용하여 누수를 방지함으로써 예측기의 정확도를 개선한 것이 가장 큰 특징입니다. 또, 의사결정나무를 적합할 때에 각각의 자식 노드에서 동일한 기준으로 분할을 하는 대칭 트리(symmetric tree) 방식을 사용하여 트리 구조를 단순화하고 계산 속도를 개선한 것도 특징입니다. 이러한 대칭 구조는 병렬연산이 가능하게 만들어 계산 속도를 높이는 데에도 기여합니다. CatBoost는 범주형 변수의 분류에 특화된 모형으로 알려져있고, 범주형 데이터를 전처리하는 과정이 내장되어있어 데이터 파이프라인 구축이 간단하다는 장점이 있습니다. 또한 결과 측면에서 LightGBM이나 XGBoost보다 안정적인 성능을 내는 경우가 많다는 평가를 받고 있습니다.

---

### 15.5. 결론

의사결정나무는 단순한 기준으로 데이터를 분할하여 최종적인 분석의 목적을 달성한다는 측면에서 매우 직관적이고 단순하면서도 성능 측면에서 강력한 분석 방법입니다. 특히, 기존의 알고리즘을 그대로 이용하는 것보다는 과적합을 방지하기 위한 여러 가지 기법이 추가된 분석 방법이 많이 사용되고 있습니다. 

변수 간의 비선형 관계도 효과적으로 모델에 담을 수 있고, 데이터 정규화나 표준화, 인코딩 등 복잡한 전처리가 필요없을 뿐만 아니라 랜덤 포레스트나 부스팅 기법 중에서는 자체적으로 결측치를 처리하는 알고리즘을 내장하고 있는 경우가 많아 적은 수의 파라미터 튜닝만으로도 매우 강력한 성능을 낼 수 있다는 장점이 있습니다. 또한 속성 중요도, OOB 오차 등 모델의 특성을 객관적으로 평가할 수 있는 지표가 광범위하게 사용되고 있다는 점에서도 "블랙박스"라는 한계 극복에 유리한 점이라고 할 수 있습니다.

반면, 의사결정나무가 가지는 근본적인 과적합 경향과 불안정성은 아무리 모형이 발전해도 제거되지 않는 한계라고 할 수 있습니다. 이보다 예측의 측면에서 더 치명적인 단점은 외삽(extrapolation)이 불가능하다는 점입니다. 기존에 존재하는 데이터를 기준으로만 최종적인 예측을 수행하기 때문에 기존의 데이터가 포섭하지 못하는 영역의 새로운 데이터에 대해서는 전혀 예측을 할 수 없다는 단점이 있습니다. 

따라서, 다른 분석기법과 마찬가지로 분석의 목적, 데이터의 성격에 대한 정확한 이해를 바탕으로 의사결정나무 기반의 분석 기법을 활용하는 것이 중요합니다.

