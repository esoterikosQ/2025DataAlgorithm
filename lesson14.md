## 14. 신경망 심화 모형

이번 시간에는 바닐라 NN과 다른 형태의 신경망을 살펴보겠습니다. 모든 신경망이 바닐라 NN과 개념적 구조는 동일하지만 노드 내부의 연산이나 각 레이어에서 데이터 처리 방법에 차이가 있습니다. 

### 14.1. CNN (Convolutional Neural Network, 합성곱 신경망)

#### 14.1.1. 이미지 데이터의 특성

이미지 데이터는 (가로 $\times$ 세로) 2차원 형태의 데이터입니다. 그런데 이 데이터는 관계형 데이터(일반적인 관측치-변수 형태, 파이썬과 R에서 데이터프레임)처럼 처리할 수 없습니다. 이미지 데이터의 가로와 세로는 관측치와 변수의 관계가 아니기 때문에 입력자료의 세로에 해당하는 차원을 정의할 수 있는 방법이 없습니다. 따라서 이미지 데이터 처리를 위해서는 일반적으로 이미지의 각 픽셀에 해당하는 데이터값을 일렬로 연결할 1차원 벡터로 변환하는 방식을 씁니다. 예를 들어 28×28 크기의 흑백 이미지는 784차원 벡터가 되는 식입니다. 

그런데, 이 과정에서 다음과 같은 문제가 발생합니다:

**공간 정보 손실** 2차원 데이터가 가지는 픽셀 간의 인접성에 관한 정보가 사라집니다. 즉, 세로로 그려진 선이 있는 이미지를 벡터로 변환하면 같은 줄에 거쳐 존재하는 동일한 값의 관계가 데이터 자체에 표시되지 않습니다. 

**파라미터 폭발** 컬러 이미지의 경우 3차원입니다. RGB의 각 채널에 해당하는 값을 픽셀 전역에 거쳐 가지고 있기 때문입니다. HD 이상의 고해상도 이미지인 경우 해상도도 크고, 컬러 이미지의 경우 차원이 하나 더 생기기 때문에 벡터로 변환한 경우 입력 벡터의 차원이 기하급수적으로 증합니다. 예를 들어, UHD 이미지의 해상도는 3840×2160이고, RGB 컬러 이미지인 경우 3채널이므로 총 3840×2160×3 = 24,883,200 차원이 됩니다. 바닐라 NN의 첫 은닉층에 이 입력을 모두 연결하려면 모형의 크기가 지나치게 커집니다.

**위치 불변성 부재** 이미지는 형태가 같은 경우 캔버스의 위치의 변화가 가지는 의미가 크지 않습니다. 이런 경우에는 형태의 유사성과 위치의 유사성을 각자 다른 특성으로 정의할 수 있어야 합니다.

신경망을 이용하여 이와 같은 이미지의 특성을 처리할 수 있도록 고안한 형태가 합성곱 신경망(CNN)입니다. 

#### 14.1.2. 합성곱(Convolution) 연산

[참고자료](https://wikidocs.net/64066)

CNN의 노드는 단순한 선형연산을 하는 것이 아니라 "합성곱 연산"으로 정의되는 연산을 수행합니다. "커널"이라고 불리는 작은 행렬(예: 3×3, 5×5)을 입력 이미지에 순차적으로 적용하면서 원소끼리 곱한 후 더하는 방식입니다. 아래와 같이 정의할 수 있습니다.

$$y_{i,j} = \sigma\left(\sum_{m=1}^{k} \sum_{n=1}^{k} w_{m,n} \cdot x_{i+m, j+n} + b\right)$$

합성곱 연산을 거친 값을 원본 데이터와 유사하게 2차원 좌표에 표시하는데, 이렇게 생성한 결과물을 특성맵(feature map)이라고 합니다. $(i, j)$는 특성맵에서 위치를 나타내는 좌표값이고, $k$는 커널의 크기를 타나냅니다. 각 커널이 가지는 가중치는 $w_{m,n}$로 표시했고, $b$는 편향값입니다.

합성곱이 이루어지는 과정은 [다음의 그림](https://wikidocs.net/64066)을 통해 확인할 수 있습니다.

CNN은 연산 과정에서 다차원에서 입력과 출력이 이루어지기 때문에 "차원"에 대한 이해가 필요합니다. 입력 데이터의 차원은 (세로 해상도 × 가로 해상도 × RGB 채널 수)로 표현할 수 있습니다. 이러한 데이터에 커널을 이용하여 합성곱을 수행하면 (커널의 해상도 × 채널 수)에 대해서 하나의 실수값이 출력으로 나옵니다. 하나의 커널이 전체 이미지의 처음부터 끝까지를 순회하고 하면 순회의 폭(stride)에 따라서 최초의 이미지와 동일한 차원이거나 더 낮은 차원의 특성맵이 생성됩니다. 이런 커널이 여러 개 존재하면서 각각의 커널이 다른 가중치와 편향값을 가지고 전체 이미지를 처음부터 끝까지 순회하는 과정이 완료되면 최초의 이미지 데이터는 (세로 해상도 × 가로 해상도 × 커널의 개수) 차원의 데이터로 바뀝니다.


---

#### 14.1.3. 합성곱 연산의 장점
"순회"라는 과정이 있어 연산이 반복되고 복잡해보이지만, 실제로 합성곱을 통해 이미지 데이터를 처리하기 위해서는 바닐라 NN과 비교했을 때에 훨씬 적은 수의 노드가 필요합니다. 가령, (100 × 100 × 3) 크기의 이미지 데이터가 있다면 이를 바닐라 NN에서 처리하기 위해서는 첫 번째 은닉층에서 3만개 이상의 노드가 필요해집니다. 반대로, (3 × 3) 커널 32개를 이용하여 합성곱을 수행하는 경우 (3 × 3 × 3) × 32 = 864개의 파라미터만으로 동일한 크기의 이미지를 처리할 수 있습니다. 이러한 파라미터 수의 차이로 인해 합성곱 연산을 수행하면 바닐라 NN에서 처리하는 것보다 이미지 데이터를 훨씬 효과적으로 처리할 수 있습니다.

또 다른 장점은, 동일한 성격의 필터가 이미지 전체를 순회하기 때문에 패턴의 위치가 변동되는 이미지의 차이를 효율적으로 처리할 수 있다는 점입니다. 가령, 이미지의 상하관계를 무시하는 바닐라 NN에서는 동일한 패턴이 이미지의 어느 위치에 존재하느냐에 따라 파라미터값이 변화해야 합니다. 반면, CNN에서는 동일한 패턴을 감지할 수 있는 커널이 이미지 전체를 순회합니다. 즉, 원형을 감지할 수 있는 커널은 이미지에서 상단에 있거나 하단에 있거나 무관하게 원형을 찾아낼 수 있습니다. 

---

#### 14.1.4. 일반적인 CNN 모델의 구성

일반적으로 CNN 모델은 합성곱, 풀링, 완전연결로 구성됩니다. 

풀링(pooling)이란 특성맵의 차원을 줄이는 연산입니다. 다양한 방법의 풀링이 존재하는데 최댓값, 평균 등을 뽑아냄으로써 커널의 크기에 해당하는 영역에서 대표값을 추출해냅니다. 풀링을 수행하면 데이터의 차원이 커널의 크기에 반비례해서 작아집니다. 즉, 정보의 손실이 발생하는데, 이 때문에 풀링의 장단점을 고려하여 모델에 적용해야 합니다. "존재" 자체가 중요한 경우에는 풀링을 통해 잃어버리는 정보가 답을 내는 데에 큰 영향이 없을 수 있고, 반대로 정확한 위치나 모양이 중요한 경우에는 풀링을 통해 중요한 정보를 손실할 수도 있습니다. 이 때문에 모델 디자인 단계에서 풀링을 어떻게 사용할지 여부를 결정합니다.

완전연결(full connect)이란 바닐라 NN과 동일한 방식으로 특성맵을 1차원 벡터로 변환하여 출력층에 연결하는 층입니다. 이 층을 통해 커널을 이용한 합성곱을 반복함으로써 이미지 데이터에서 정제한 데이터를 최종 출력을 변환합니다.

종합하면, CNN 모델은 다음과 같은 형식으로 구성됩니다.

```
입력 이미지
    ↓ [Conv Layer 1: CNN + Pooling]
저수준 특징 (edges, corners)
    ↓ [Conv Layer 2: CNN + Pooling]
중수준 특징 (textures, simple shapes)
    ↓ [Conv Layer 3: CNN + Pooling]
고수준 특징 (eyes, wheels, windows)
    ↓ [Fully Connected Layer: ReLU + Softmax]
최종 분류 (cat, dog, car)
```

바닐라 NN과 마찬가지로 은닉층이 반복될수록 커널이 잡아내는 특징이 저수준(점, 선)에서 고수준(도형, 특정한 모양)으로 추상화됩니다. 

---

### 14.2. RNN, LSTM, GRU

#### 14.2.1. 순차 데이터와 RNN의 필요성

바닐라 NN은 정형 데이터를 처리하는 데에 적합합니다. 하지만 시계열 데이터의 경우, 각 시점의 데이터가 시간적인 의존성을 가지는 것을 전제로 하고 있기 때문에 데이터의 형태와 처리 방법이 정형 데이터와 다를 수 밖에 없습니다. 특히, 시계열 데이터의 특징인 시간적 의존성을 반영하기 위해서는 각각의 레이어에서 전기의 데이터와 차기의 데이터 사이의 연결관계를 잡아줄 필요가 있습니다. 바닐라 NN에서는 이러한 특성을 반영하기 어렵습니다.

이를 보완하기 위해서 등장한 것이 RNN, LSTM, GRU와 같은 형태의 신경망 모델입니다.

---

#### 14.2.2. RNN (Recurrent Neural Network)의 노드 연산

RNN의 노드 내부에서는 다음과 같은 연산이 이루어집니다.

$$
h_t = \sigma(W_{hh} h_{t-1} + W_{xh} x_t + b)
$$

즉, t시점의 은닉 상태 $h_t$는 이전 시점의 은닉 상태 $h_{t-1}$와 현재 시점의 입력 $x_t$이 모두 노드의 출력값 생성에 기여를 합니다. 이러한 특성을 표현하기 위해서 통상 RNN은 동일한 노드가 t시점에서 t+1 시점으로 출력값을 전달하는 형태로 도식화합니다. [도식화 참고자료](https://wikidocs.net/22886)


#### 14.2.3. RNN의 한계와 LSTM, GRU

RNN은 시계열 데이터의 시간적 의존성을 표현할 수 있다는 점에서 장점이 있지만 메모리 효과와 기울기 소실/폭발에서 자유롭지 못하다는 치명적인 단점이 있습니다.

메모리 효과란, 과거 시점의 데이터가 다음 시점의 데이터에 영향을 미치는 현상을 의미합니다. RNN은 구조상 당연히 과거 시점의 데이터가 출력값에 반영이 되지만, 시점이 멀어지면 멀어질수록 영향력이 급격하게 감소합니다. 역전파 과정의 미분식을 생각해보면, 이해가 쉬워집니다. 즉, 선행 데이터의 영향력이 후행 데이터에도 미치는데, 시점의 차이가 큰 데이터의 경우 역전파 과정에서 미분값이 지속적으로 늘어나는 성질이 생깁니다. 이 때, $\frac{\partial h_t}{\partial h_{t-1}}$의 크기가 1보다 작은 경우 기울기가 점점 작아지면서 최종적으로 0에 가까워지는 기울기 소실 문제가 발생하고, 1보다 큰 경우 기울기가 기하급수적으로 커지면서 기울기가 폭발하는 문제가 발생합니다. 

이러한 문제를 해결하기 위해 등장한 것이 LSTM(Long Short-Term Memory)입니다. LSTM은 노드 내부에 입력값을 처리하는 셀, 출력값을 처리하는 셀, 망각값을 처리하는 셀 등 3개의 게이트를 두고 입력 데이터와 직전 시점에 전달받은 데이터를 동시에 처리합니다. 이 과정에서 손실함수값에 따라 어떤 정보를 망각할지를 학습할 수 있습니다. 이 때문에 LSTM은 주기의 차이가 큰 시점의 데이터라도 중요도에 따라 필요한 것은 다음 노드로 전달하고 필요하지 않은 것은 버리는 방식의 연산이 가능해집니다. [LSTM 참고자료](https://wikidocs.net/22888)

GRU(Gated Recurrent Unit)는 LSTM을 단순화하여 연산속도 측면에서 장점을 가지고 있습니다. 한국인 조경현 교수 연구팀이 제안한 것으로도 유명합니다. LSTM에서 3개의 게이트를 2개로 줄인 것이 핵심입니다. 즉, 망각과 입력 게이트를 합해서 어떤 정보를 버릴지를 결정하고(리셋 게이트), 과거 정보와 새 정보의 비율을 결정함으로써(업데이트 게이트) 3가지 방법으로 연산이 필요했던 LSTM에 비해 적은 연산으로 유사한 성능을 낼 수 있도록 한 것이 특징입니다. [GRU 참고자료](https://wikidocs.net/22889)

--- 

#### 14.2.4. 자연어 처리에 응용

시계열 데이터를 처리하는 것이 핵심인 RNN, LSTM, GRU는 자연어 처리 분야에서 널리 활용됩니다. 자연어 데이터는 데이터의 특성상 순차적으로 전개되고, 전개 과정에서 앞 시점의 단어나 표현이 다음 시점의 단어나 표현에 영향을 미친다는 점에서 시계열 데이터의 특성을 가지고 있기 때문입니다. 

RNN을 이용하는 경우는 많지 않지만 토큰 중심의 자연어 처리가 이루어지는 경우가 아니라 철자, 글자 단위에서 자연어를 처리하는 연구에서는 여전히 RNN을 응용하는 연구들이 존재합니다.

하지만 대부분의 자연어 처리 모형은 LSTM이나 GRU를 이용합니다. 특히, 길이가 긴 문서에서 장기 의존성을 파악하는 점에서 RNN에 비해 LSTM과 GRU가 월등한 장점을 가지고 있기 때문입니다.

하지만 뒤이어 서술할 트랜스포머가 등장하면서 현재의 LLM은 거의 모두 트랜스포머 구조에 기반하면서 부분적으로 필요한 경우에 LSTM이나 GRU를 혼용하는 방식으로 설계됩니다.

---

### 14.3. 트랜스포머(Transformer)

#### 14.3.1. 원리

LSTM/GRU가 기울기 문제를 완화했지만, 여전히 한계가 있습니다. 일단 데이터가 순차 입력하고 순차 처리해야하기 때문에 병목 구간이 발생합니다. 앞서 입력한 데이터의 값이 뒤에 입력하는 데이터 값에 영향을 미치기 때문에 앞선 데이터를 처리하지 않으면 뒤에 이어오는 데이터를 처리할 수 없습니다. 이는 병렬 처리가 불가능한 구조로, 데이터가 길어질수록 처리하는 속도가 매우 느려지는 문제가 생겼습니다. 

이에 RNN 계열의 노드를 전부 제거하고 어텐션 메커니즘만으로 신경망을 구성해도 자연어 처리의 문제가 해결이 가능하며 성능이나 효율성 면에서도 월등하다는 점이 구글 연구진의 연구 결과로 발표되었습니다. 이 [기념비적인 논문](https://arxiv.org/abs/1706.03762)을 통해 현대 LLM의 기반이 되는 트랜스포머 모델이 확립되었습니다.

---

#### 14.3.2. 어텐션(Attention) 메커니즘

어텐션 메커니즘은 RNN 계열의 자연어 처리 모형에서 기울기 소실이나 폭발로 인해 길이가 긴 자연어를 처리할 때에 자연어가 가지는 선후관계의 정보를 잃어버리는 문제를 해결하기 위해서 고안된 기법입니다. 핵심 아이디어는 새로운 단어를 처리하기 위해서 문장의 어느 부분에 집중해야 하는가?를 매 처리시마다 참고하는 함수체계를 만들어서 수치화한 후 이를 바탕으로 밀접한 관계가 있는 단어를 집중적으로 참고하여 단어를 처리한다는 것입니다. 

([참고자료](https://wikidocs.net/22893)는 수식과 구체적인 예시를 이용하여 어텐션 메커니즘을 소개하고 있지만, 이해가 어려우시다면 생성형 AI의 도움을 받아서 직관적으로 이해하는 데에 주력하는 편이 더 좋습니다.)

---

#### 14.3.3. 셀프 어텐션(Self-Attention)과 멀티헤드 어텐션(Multi-Head Attention)

트랜스포머 구조는 특정 단어를 처리할 때에 앞선 문맥에서 등장했던 다른 단어와의 관계를 수식화한 어텐션 메커니즘의 개념을 확장하여 셀프 어텐션을 통해 자연어 데이터를 처리하는 방식을 따르고 있습니다.

즉, 순차적으로 문장의 각 구성요소를 이루는 단어들이 입력되면 순차 입력된 데이터를 처리하는 방식이 아니라 전체 문장을 한꺼번에 입력하고 이들 문장을 이루는 전체 단어들이 자기자신을 포함한 모든 단어들과 어떤 관계가 있는지를 보여주는 함수체계를 셀프 어텐션이라고 합니다. 이를 통해 입력이 순차적으로 이루어지지 않고 전체 자연어 데이터가 통으로 들어갈 수 있는 구조를 구현했습니다.

멀티헤드 어텐션(multi-head attention)은 이와 같은 셀프 어텐션을 여러 개를 병렬적으로 처리하는 방식을 의미합니다. 개념적으로 이해하면 셀프 어텐션의 1번은 문법 구조, 2번은 서술 구조, 3번은 수식 구조 등 각자 다른 관점에서 자연어 간의 상호관계를 반영하는 어텐션을 병렬적으로 처리하는 방식입니다. 이를 통해 GPU를 이용한 병렬연산이 가능해지고 RNN 계열의 신경망보다 월등히 빠른 속도를 구현할 수 있게 되었습니다.

[트랜스포머의 구조와 작동 원리에 대해서는 이 자료를 참조하세요.](https://wikidocs.net/31379)

---

### 14.4. 맺음말

금주 학습자료를 통해서 신경망의 발전 과정에서 등장한 주요 모델들을 개괄적으로 살펴봤습니다. (직관적인 개념 확립을 위해 수식을 이용한 설명은 최대한 피했습니다. 다만, 이런 방식으로 이해하는 경우 모델이 정확하게 어떻게 작동하고 내가 가진 데이터를 처리하기 위해서는 모델을 어떻게 설계해야 하는지에 대한 방향이 정립되지 않습니다. 실제 데이터 처리에서 구현하기 위해서는 반드시 첨부한 참고자료를 이용하여 수식을 통한 이해를 확립하시기를 권장드립니다.)

신경망의 발전은 데이터가 가지는 특성을 처리하기에 유용한 방향을 고안하는 과정이라고 할 수 있습니다. 국소적 패턴을 가지는 이미지 데이터 처리를 위한 CNN, 시간적 의존성을 가지는 시계열 데이터 처리를 위한 RNN/LSTM/GRU, 순차적 입력의 한계를 극복한 트랜스포머 등이 그 예입니다. 이러한 신경망 모형의 발전은 "블랙박스"라고 표현하는 신경망의 특성을 무시한 것이 아니라, 역설적으로 신경망의 각 층, 각 노드가 처리하는 데이터의 특성을 정확하게 이해하고 이를 개선, 활용하는 방향으로 이루어졌다는 점에 주목할 필요가 있습니다.

데이터 분석을 수행하는 연구자, 분석가로서 신경망 모형의 정확한 개념과 특성을 이해하고 어떤 데이터 처리에 어떤 기법을 사용하는 것이 좋을지를 판단하는 능력이 무엇보다 중요한 이유입니다.