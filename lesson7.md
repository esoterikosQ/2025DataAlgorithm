## 7. 군집분석

### 7.1. 개관

군집분석이란 전체 관측된 데이터를 유사도를 기준으로 일정한 숫자의 그룹으로 묶는 분석 기법이다. 군집분석은 분석의 개념과 과정이 매우 직관적이라는 점에서 다변량 데이터를 분석하기 위해 애용된다. 군집화를 위한 유사도/비유사도의 기준이 각각의 관측치가 된다는 측면에서 MDS와 유사하지만 MDS가 측도체계 자체를 만들어내는 데에 목적이 있는 반면 군집분석은 명목변수로서 각 군집의 번호를 추출한다는 점에서 차이가 있다. 금주 수업에서는 군집분석의 원리를 알아보고, 유사도를 측정하는 다양한 방법을 확인한다. 그리고 군집분석의 다양한 방법 중에서 위계적 군집, K평균 군집, 밀도기반 군집 방법으로 DBSCAN을 살펴본다.

#### 7.1.1. 원리

군집분석은 전체 데이터를 k개의 군집cluster으로 구분하기 위해 여러 가지 방법을 사용하는데, 기본적으로는 군집 내에 속해있는 관측치 간의 거리를 극소화하고 군집 사이의 거리를 극대화하는 것을 분석의 목표로 한다. 이를 수식으로 표현하면 아래와 같다.

```math
T = W(C) + B(C) \\

T = \frac 1 2 \sum_{i = 1}^n \sum_{j = 1}^n d_{ij} = \frac 1 2 \sum_{k = 1}^K \sum_{C(i) = k} \bigg ( \sum_{C(j) = k} d_{ij} + \sum_{C(j) \neq k} d_{ij} \bigg) \\

B(C) = \frac 1 2 \sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) \neq k} d_{ij}

```
위 식은 다음과 같이 해석할 수 있다.

관측치 간의 전체 거리의 합(T)은 군집 내 관측치 간의 거리(W)와 군집 간 거리(B)로 나눌 수 있다. ($T = W + B$) 군집 내 거리와 군집 간 거리는 각 관측치를 어떤 군집에 포함시키느냐(C)에 따라 결정되는데, 따라서 W, B는 C의 함수로 표현할 수 있다. ($W = W(C), B = B(C)$) 총 거리는 모든 관측치 간의 상호거리의 합으로 표현할 수 있는데($\frac 1 2 \sum_{i = 1}^n \sum_{j = 1}^n d_{ij}$), 이는 다시 i번째 관측치와 같은 군집에 속한 관측치와의 거리($\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) = k} d_{ij}$)와 다른 군집에 속한 관측치와의 거리($\sum_{k = 1}^K \sum_{C(i) = k} \sum_{C(j) \neq k} d_{ij}$)로 분해할 수 있다. 이 경우, 군집 간 거리는 소속 군집이 다른 관측치와의 거리의 총합으로 정의할 수 있다. 

모든 군집분석의 목적은 W(C)로 표현되는 손실함수를 최소화하는 것이다. (데이터셋이 주어진 경우 T는 불변이기 때문에 W를 최소화하면 B가 최대화됩니다.)

#### 7.1.2. 다양한 거리측도

군집분석을 이해하기 위해서는 가장 중요한 것은 거리에 대한 개념이다.

n개의 관측치를 가진 데이터는 각각의 관측치 사이의 유사도similarity 또는 상이도dissimilarity를 기준으로 거리를 정의할 수 있다. 이때 거리 행렬(distance matrix 또는 proximity matrix)을 다음과 같이 정의할 수 있다.

```math
\boldsymbol D = \begin{pmatrix}

d_{11} & d_{12} & d_{13} & \cdots & d_{1n} \\

d_{21} & d_{22} & d_{23} & \cdots & d_{2n} \\

&& \vdots \\

d_{n1} & d_{n2} & d_{n3} & \cdots & d_{nn} \\

\end{pmatrix}
```

이때 행렬의 대각원소는 0이 된다. 군집분석에서는 위와 같은 **거리 행렬 D**가 기본적으로 군집화를 위한 입력 데이터가 되고, n개의 관측치 각각에 대해 정해진 **군집 번호 k**가 출력 데이터가 된다. 따라서 앞서 표현한 군집화의 방법에 관한 함수 C는 $C(D) = K$로 표현할 수 있다. (K는 $n \times 1$차원의 벡터)

i번째 관측치와 j번째 관측치의 거리 $d_{ij}$는 아래와 같이 표기할 수 있다.

```math
d_{ij} = d(x_i, x_j)
```

$d (x_i, x_j)$는 거리를 정의하는 함수로, h번째 변수에 대한 i번째 관측치와 j번째 관측치의 거리를 입력값으로 받아 실수값을 출력한다. $d()$를 어떻게 정의하느냐에 따라 여러 가지 종류의 거리 개념이 존재한다. 


##### 7.1.2.1. 유클리드 거리

유클리드 거리는 기하학적인 거리로, 가장 직관적이고 가장 널리 쓰이는 거리의 개념이다. 변수가 p개인 데이터셋에서 i번째 관측치와 j번째 관측치의 거리는 아래와 같이 표현할 수 있다.

```math
d(x_i, x_j) = \| x_i - x_j \|^2_2 = \sum_{h = 1}^p (x_{ih} - x_{jh})^2
```

유클리드 거리는 직관적이고 연속인 볼록함수 형태이기 때문에 수학적으로 간결한 장점이 있으나 편차의 제곱에 비례하는 특성을 가지기 때문에 이상치에 민감해서 소표본 데이터에서 왜곡을 낳을 수 있다는 단점이 있다.

K평균 군집은 유클리드 거리를 기반으로 한다.

> 참고 : 엄밀하게 말하면 유클리드 거리는 위에서 정의한 거리체계에 제곱근 값을 씌운 것으로 정의합니다. 다만, 교재에 따라서 제곱근을 씌우거나 안 씌운 값이 혼용되는 경우가 많기도 하고, 거리값 자체가 아니라 거리값을 응용한 다양한 분석 방법에서는 계산상의 편의 때문에 제곱근을 씌우지 않은 값을 사용하는 경우도 많아서 제곱근을 씌우지 않았습니다. 제곱근을 씌운 거리값은 $\| x_i - x_j \|_2$라고 구분해서 표기합니다.

##### 7.1.2.2. 맨하탄 거리

유클리드 거리만큼 널리 사용하는 거리로, 유클리드 거리가 가지는 이상치로 인한 왜곡에서 자유롭다는 장점이 있다. 또한 저차원과 고차원에서의 거리 변화가 발생하지 않아 다변량 데이터의 거리를 안정적으로 계산할 수 있다. 하지만 맨하탄 거리는 불연속 함수의 형태로 정의되기 때문에 해의 존재성이나 유일성을 보장하지 않는다는 단점이 있다.

```math
d(x_i, x_j) = \| x_i - x_j \| = \sum_{h = 1}^p | x_{ih} - x_{jh} |
```

참고로, 노름의 차원 수에 따라 거리를 일반화한 개념이 **체비셰프 거리**로 아래와 같이 표기할 수 있다.

```math
d(x_i, x_j) = \| x_i - x_j \|_l = \bigg ( \sum_{h = 1}^p (x_{ih} - x_{jh})^l \bigg )^{\frac 1 l}
```

이때 l = 1이면 맨하탄 거리이고, l = 2이면 유클리드 거리이다.


##### 7.1.2.3. 표준화 거리

다변량 데이터에서 각 변수를 표준화한 유클리드 거리를 표준화거리라고 한다. 표준화 거리는 변수의 단위 차이로 인한 거리의 왜곡을 방지하기 위해 사용한다. 다만, 변수들 간의 독립을 가정한 거리 계산법이라는 점에서 후술할 상관계수 거리나 마할라노비스 거리와 대조적이다.

```math
d(x_i, x_j) = \sqrt {\sum_{h = 1}^p \bigg ( \frac { x_{ih} - \mu_h }{ \sigma_h } \bigg )^2}
```

다변량 데이터셋의 표준화 거리를 행렬식으로 표현하면 다음과 같다.

```math
d(\boldsymbol X) = \sqrt { \boldsymbol X D^{-1} \boldsymbol X'}
```

이때, $\boldsymbol D$는 각 변수의 분산을 대각원소로 가지는 대각행렬이다.

##### 7.1.2.4. 상관계수 거리

상관계수 자체를 거리의 측도로 사용하기도 한다. 아래와 같은 피어슨 상관계수 값을 활용하여 거리체계를 정의하는 방법으로,

```math
r(x_i, x_j) = \frac {\sum_h ( x_{ij} - \bar {x_i} )( x_{jh} - \bar {x_j} )} {\sqrt { \sum_h (x_{ih} - \bar {x_i})^2 \sum_h (x_{jh} - \bar {x_j})^2} }
```

상관계수 거리는 다음과 같이 정의할 수 있다.

```math
d(x_i, x_j) = \sqrt{ 2(1 - r(x_i, x_j))}
```

이때의 상관계수는 개별 관측치쌍 간의 상관계수이다. 상관계수를 거리로 사용하는 경우 변수가 가지는 고유한 단위가 사라지고 두 관측치가 가지고 있는 벡터값의 패턴만 남게 된다. 그래서 변수의 단위나 크기는 중요하지 않고 관측치 간의 패턴이나 형태가 얼마나 유사한지만을 고려할 때에 상관계수 거리를 사용한다. 주로 시계열 데이터나 측정시점 t를 관측치의 단위로 하는 프로파일 데이터에서 많이 이용한다.

상관계수를 비모수적 상관계수인 스피어만 상관계수나 켄달의 타우($\tau$)를 사용하는 경우 비연속형 데이터에 대해서도 거리를 정의할 수 있다. 이 경우 모수적 상관계수 - 비모수적 상관계수와 동일하게 이상치, 변수의 단위로 인한 거리의 왜곡을 방지할 수 있다는 장점이 있다.

##### 7.1.2.5. 마할라노비스 거리

표준화 거리가 변수의 단위를 제거한 거리라면 마할라노비스 거리는 변수 간의 공분산 구조까지 반영한 거리이다. 

```math
d(x_i, x_j) = \sqrt { (\boldsymbol x_i - \boldsymbol x_j)' \Sigma^{-1} (\boldsymbol x_i - \boldsymbol x_j)}
```

다변량 데이터셋의 마할라노비스 거리를 행렬식으로 표현하면 다음과 같다.

```math
d(\boldsymbol X) = \sqrt { \boldsymbol X \Sigma^{-1} \boldsymbol X'}
```

이때, $\boldsymbol \Sigma$는 분산/공분산 행렬이다.

마할라노비스 거리는 변수 간의 상관관계를 보존하는 것이 군집화의 주된 관심사일 때에 사용하는 거리이다. 자연어 분석에서 잠재 디리클레 할당(LDA) 분석 방법에 사용한다.

##### 7.1.2.6. 자카드 거리

앞선 거리 측도가 연속형 변수값을 가지는 데이터에만 적용할 수 있는 거리였지만 명목형 변수값을 가지는 데이터에 대해 적용할 수 있는 거리가 자카드 거리, 코사인 거리, 순위상관계수이다. 

자카드 거리는 자카드 유사도를 거리 개념으로 바꾼 것이다. 자카드 유사도는 다음과 같이 정의할 수 있다.

```math
J(x_i, x_j) = \frac { | X_i \cap X_j | } {| X_i \cup X_j |}
```

이범주형 데이터의 경우 자카드 유사도는 쉽게 계산할 수 있고, 다범주형 데이터의 경우 원-핫 인코딩 후 계산할 수 있다.

자카드 유사도를 기반으로 하는 자카드 거리는 다음과 같이 정의된다.

```math
d(x_i, x_j) = 1 - J(x_i, x_j)
```

자카드 거리는 [0, 1]사이의 값으로 정의된다. 자카드 거리는 존재와 부재가 데이터의 중요한 정보일 때에 정보의 손실없이 거리를 정의할 수 있는 거리체계로 알려져 있다. 또, 해석이 직관적이고 거리 계산이 쉽다는 장점이 있다. 따라서 자연어 처리기법 중에서 문서 행렬이나 BoW(Bag of Word)를 기반으로 의미를 분석하는 경우에 자주 활용한다. 


##### 7.1.2.7. 코사인 거리

코사인 거리는 코사인 유사도를 기반으로 계산하는 거리이다. 코사인 유사도는 아래와 같이 계산한다.

```math
C(x_i, x_j) = \frac { x_i' x_j } {\| x_i \| \| x_j \|} = cos \theta
```

$\| x_i \|$는 벡터 $\boldsymbol x_i$의 단위벡터로, $\| x_i \| = \sqrt { \sum_h x_{ih}^2 }$으로 정의된다. 유클리드 평면에서 코사인 유사도는 두 벡터의 방향 차이를 나타내는 각도에 대한 코사인 값과 같아진다.


코사인 유사도를 기반으로 하는 코사인 거리는 다음과 같이 정의할 수 있다.

```math
d(x_i, x_j) = 1 - C(x_i, x_j)
```

코사인 거리값은 [0, 2] 사이의 값을 가진다. 코사인 거리는 상관계수 거리와 유사하게 관측치 간의 관계의 방향, 형태만을 고려한 거리체계이다. 따라서 변수가 가지는 단위나 크기의 차이는 거리값에 반영되지 않으며, 자카드 거리와 다르게 연속형과 범주형 변수가 혼합된 경우에도 거리값을 정의할 수 있다. 상관계수 거리와 유사하게 프로파일 데이터나 자연어 데이터와 같이 크기보다는 연관성의 형태가 중요한 경우에 자주 사용하는 거리체계이다.

---

#### 7.1.3. 군집 간 연결linkage

후술할 위계적 군집(hierarchical cluster)과 같이 "관측치 $\rightarrow$ 군집"의 방향으로 군집을 묶어가는 군집분석 기법에서는 군집 간의 연결을 어떻게 정의할 것인지에 따라 군집화의 결과가 달라진다. 본 절에서는 군집분석과 관련하여 가장 일반적으로 언급되는 군집 간 연결 방법을 소개한다.

##### 7.1.3.1. 최단거리법(nearest-neighbor / single linkage)

두 군집 사이에서 거리가 가장 짧은 관측치를 기준으로 군집 간의 거리를 평가하는 방법으로 아래와 같이 표현할 수 있다.

```math
d(G, H) = \min_{i \in G, j \in H} d_{ij}
```

최단거리법으로 군집화하는 경우 군집이 가늘고 길게 이어지는 형태가 되는 것으로 알려져있다.

##### 7.1.3.2. 최장거리법(furthest-neighbor / complete linkage)

최단거리법과 반대로 군집 내에서 거리가 가장 긴 관측치를 기준으로 거리를 평가하는 방법이다. 

```math
d(G, H) = \max_{i \in G, j \in H} d_{ij}
```

최장거리법으로 군집화하는 경우 조밀하고 구형으로 군집이 생성되며 이상치에 덜 민감한 것으로 알려져있다.

##### 7.1.3.3. 중심연결법(centroid linkage)

군집의 중심점을 설정하여 두 군집 사이의 거리를 평가하는 방법이다. 후술할 K평균 군집은 중심연결법을 군집화 결과를 평가하는 지표로 사용한다.

```math
d(G, H) = \| \mu_G - \mu_H \|
```

중심연결법으로 군집화하는 경우 **역전현상inversion**이 일어날 수 있다. 역전현상이란, 군집화 과정에서 후순위에서 병합하는 군집의 거리가 선순위 병합의 거리보다 짧아지는 현상을 의미한다. 이러한 현상이 생기는 이유는 중심점의 존재에서 데이터 관측치의 존재성이 보장되지 않기 때문이다. 즉, 중심점은 군집 내의 관측치 간의 거리에 바탕을 두고 지정하는 가상의 점이기 때문에 군집 내에 새로운 관측치가 추가되면 중심점이 갱신된다. 그런데 특정 조건에서는 중심점의 위치가 다음에 추가할 관측치의 방향으로 이동할 수 있고, 이 경우 선순위 병합시의 군집 간 거리보다 후순위 병합시의 군집 간 거리가 짧아지는 현상이 생긴다. 

중심연결법으로 군집을 생성하더라도 군집 생성결과 자체에 오류가 생기지는 않지만, 생성된 부모노드와 자식노드의 관계 간 일관성이 깨지기 때문에 해석의 어려움이 생긴다. 이 때문에 군집화 기준치의 단조성이 필요한 경우에는 중심연결법을 사용하지 않는다.

##### 7.1.3.4. 평균연결법(average linkage)

평균연결법은 군집 내 모든 관측치 간의 거리의 평균을 군집 간의 거리로 해석하는 방식이다. 

```math
d(G, H) = \frac 1 {n_G n_H}\sum_{i \in G} \sum_{j \in H} d_{ij}
```

여기서 $n_G, n_H$는 각각 군집 G와 H에 속해있는 관측치의 개수이다.

평균연결법으로 군집화하는 경우 최단거리법과 최장거리법의 중간 정도의 성질을 가진 것으로 알려져있다.

##### 7.1.3.5. 와드연결법(Ward's method, minimum-variance)

군집 내 관측치 간의 제곱합을 계산하여 제곱합을 기준으로 군집화하는 방법이다. 평균으로부터 관측치까지의 편차를 '불순도'로 간주하고 새로운 관측치가 편입되었을 때에 군집의 불순도가 가장 작아지는 방향으로 군집을 확장하는 기법이다.

```math
d(G, H) = \Delta (G, H) = \frac {n_G n_H}{n_G + n_H} \| \mu_G - \mu_H \|^2_2
```

와드연결법에 따른 거리값을 해석하면, $\frac {n_G n_H}{n_G + n_H}$항의 존재 때문에 개체가 적은 군집일수록 병합이 쉬워지고, 군집 간의 평균 차이가 적은 군집일수록 병합이 쉬워진다. 

와드연결법은 제곱합이 커지지 않도록 억제하는 방향으로 군집화가 이루어지기 때문에 군집이 조밀해지고 구형에 가까워지는 것으로 알려져있다. 또한 군집의 크기에 영향을 받는 연결방식이기 때문에 작은 군집이 큰 군집에 쉽게 흡수되는 것으로도 알려져있다.

와드연결법은 유클리드 거리를 기반으로 하고 있기 때문에 비유클리드 거리체계에서는 이론적인 성질이 깨질 수 있다.

---

### 7.2. 위계적 군집(hierarchical clustering) : 군집의 체계화

실무에서 군집분석을 활용할 때에 가장 큰 과제는 군집의 개수 k를 정하는 것이다. 외생적인 요인으로 인해 k의 수가 정해져있는 경우(영업직 사원 k명에게 전체 고객을 할당하는 문제, 이미지를 최소화할 수 있는 차원 k를 구한 뒤 벡터양자화 기법으로 이미지를 압축화하는 문제 등)에는 군집분석의 초점이 **어떤 방식으로** 군집화하고 **어떻게 평가**하느냐에 맞춰지지만, k의 수를 정해야하는 경우에는 **k를 어떻게 정하느냐**에 맞춰질 수 밖에 없다. 어떤 군집화 방법을 사용하더라도 k의 수는 분석가가 설정해야 하기 때문이다. 또한, 군집분석은 적절한 군집의 수나 분류 방법이 정해져있지 않은 비지도학습 계열의 분석 방법이기 때문에 다른 지도학습 계열의 분석 방법에서 사용하는 교차검증을 통한 초모수 선택이 불가능하다.

k를 모르고 군집화를 해야하는 경우, 적정한 k의 수를 확인하기 위한 방법 중 하나는 다양한 수준의 k에 따른 군집화를 진행한 후 적절한 형태, 적절한 개수의 군집이 나오는 수준에서 k를 결정하는 것이다. 이러한 종류의 접근법에 가장 적합한 군집분석 방법이 위계적 군집이다.

위계적 군집은 아래의 알고리즘을 거쳐 계산한다.

1. 거리 행렬 D를 계산

2. $d_{ij}$값이 가장 작은 두 관측치를 하나의 군집으로 묶어 군집 $C_k$를 생성

3. $C_k$와 나머지 관측치 간의 거리(군집 간 거리)를 다시 계산해서 군집 간 거리가 가장 거리가 짧은 관측치와 묶은 군집 $C_{k+1}$을 생성

4. 3을 n-1회 반복

이때에 어떤 $d_{ij}$, 어떤 군집 간 거리를 사용할 것인지는 전적으로 분석가가 결정해야할 문제이다. 거리의 정의, 군집 간 거리의 정의에 따라 동일한 데이터를 군집화했을 때의 결과는 크게 달라진다. (마찬가지로 데이터의 관측치가 늘어나고 줄어듦에 따라 전체 군집의 체계가 매우 달라지기 때문에 유동적인 데이터에 대해서는 군집분석을 적용하는 것이 어렵다.) 일단 주어진 데이터와 거리체계에 따라 군집화가 이루어지고 나면 결과에 대한 해석을 해야하는데, 군집의 개수 k와 관련하여 직관적인 이해도가 가장 높은 것이 위계적 군집의 가장 큰 장점이다.

데이터셋 전체가 하나의 군집이라고 본다면 이 군집을 2개로 분할한다는 것은 하나의 군집을 2개로 나눌 수 있는 모종의 기준이 있다는 뜻이고, 이 기준에 따라서 이질적인 하위 집합을 만들어낼 수 있다. (위계적 군집에서는 이 기준을 **거리**로 정의한다.) 마찬가지로 4개의 군집을 3개로 줄인다고 했을 때에 기준에 맞지 않는 어느 군집이 다른 군집과 병합되어 1개의 군집이 줄어든다고 생각하는 것이 직관적이다. (1개의 군집이 어떤 기준에 따라서 각각의 군집으로 흩어질 수 있다고 생각할 수도 있지만, 이 경우에는 4개의 군집이 생성되었다는 전제와 맞지 않습니다. 그렇게 흩어질 수 있는 군집이었다면 애초에 생성될 수가 없고, 역으로 3개의 군집을 유사한 방법으로 4개의 군집으로 나눈다는 것은 애초에 3개의 군집을 정의하는 방법 자체에 오류가 있었다고 생각하는 것이 가장 직관적이고 이해하기 쉽습니다.) "군집"이라는 개념에 대해 생각할 수 있는 가장 단순하고 직관적인 이해에 걸맞는 결과물을 보여주는 것이 위계적 군집이다. 즉, 위계적 군집의 결과에서는 전체 n개의 데이터를 1개에서 n-1개의 군집으로 병합하고 분할하는 과정 자체가 매우 일관성있게 나타난다. 이를 확인할 수 있는 시각화 기법이 덴드로그램이다.

위계적 군집의 약점은 연산량이 매우 크다는 점이다.

n개의 관측치를 가진 데이터셋을 **전수 탐색** 방법으로 K개의 군집으로 나눈다고 가정했을 때에 관측치쌍의 조합에 따른 손실함수값을 계산하기 위한 연산량은 다음과 같다.

```math
S(n, K) = \frac 1 {K!} \sum_{k = 1}^K (-1)^{K-k} \binom K k k^n
```

10개의 관측치를 4개의 군집으로 나누기 위해서 거쳐야하는 연산횟수는 34,105회이고, 관측치가 19개로 늘어나는 경우 연산량은 $10^{10}$에 달한다. (ESL, 2ed, pp. 508-9)

위계적 군집을 생성하기 위해서도 비슷한 과정을 거치는데, 거리값을 계산해야하는 모든 쌍(최대 $n^2$ 종류)의 거리값을 계산한 후 최적의 군집을 생성하는 연산을 최대 n-1회 반복한다. 다만, 이미 군집으로 편입된 경우에는 관측치 간의 거리를 별도로 다시 계산하지 않기 때문에 전수 탐색 방법에 비해서는 연산량이 적다. 조합적 탐색의 경우 시간복잡도가 $\Omega(S(n, K)nd)$를 따르지만, 위계적 군집은 거리 계산의 방법에 따라 $O(n^2) \sim O(n^3)$을 따르기 때문에 연산량이 훨씬 적다.

하지만, 위계적 군집 역시 데이터의 크기에 지수적으로 비례한다는 점에서 연산량이 적다고 하기는 어렵다. 이 점은 후술하는 K평균 군집과 비교했을 때에 더욱 두드러진다.


---

#### 7.2.1. 덴드로그램(수형도, dendrogram)

위계적 군집의 결과물은 덴드로그램으로 확인할 수 있는데, 군집 간의 위계관계가 불분명한 다른 군집화 방법에 비해서 덴드로그램을 통한 높은 해석성은 위계적 군집이 가지는 큰 장점이다. 

덴드로그램은 적절한 군집의 개수를 결정하기 위한 직관적인 근거를 제공해주기도 한다. k를 결정하기 위한 아무런 제약이나 기준이 없는 상태라면 덴드로그램에서 분리한 각 군집의 거리와 크기를 적절하게 고려해서 가설적인 k의 개수를 정할 수 있다. 

각자 다른 군집 간 거리체계에 따라 생성한 위계적 군집의 특성을 덴드로그램을 통해서 확인할 수도 있다. 수리적인 근거가 없이 분석의 결과물을 평가해야하는 상황이라면 도메인 영역의 업무지식을 결합해서 어떤 방식의 군집화가 나은지를 판단할 수 있다.

하지만, 덴드로그램을 결정론적으로 해석하는 것은 매우 위험하다. 전술한 바와 같이 군집분석의 근원적인 한계가 사소한 데이터의 증감에 전체 분석의 결과가 급격하게 달라질 수 있다는 점이다. 따라서 덴드로그램은 정해진 데이터셋을 정해진 방법으로 군집화했을 때의 결과로 단편적으로 이해하는 것이 가장 안전하고, 그 선을 넘어서 알고리즘의 특성이나 데이터셋의 내재적인 특성이라고 결론짓기 위해서는 군집분석 외에도 매우 많은 전제와 다른 근거가 필요하다.

---

### 7.3. K평균 군집 : 적은 연산량, 압도적인 편의성

#### 7.3.1. 원리

K평균 군집은 군집의 수 k가 정해져있다는 가정 하에 알고리즘을 이용하여 연산량을 축소하는 분석기법이다. 이때에 사용하는 거리는 유클리드 거리이다.
K평균 군집을 생성하는 알고리즘은 아래와 같다.

```
1. 랜덤하게 선정한 k개의 개체를 초기 군집 중심점centroid으로 정의한다.

2. 각각의 관측치를 가장 가까운 중심점을 기준으로 군집화한다.

3. 군집화된 관측치군의 중심점을 다시 정의한다.

4. 2, 3에서 변화가 없을 때까지 2~3을 반복한다.
```

위 알고리즘에서는 중심점의 개수와 관측치의 수를 곱한 수만큼을 각각의 알고리즘이 반복되는 횟수마다 반복해서 이루어진다. 이 경우 시간복잡도는 $nO(nkp)$를 따른다. 관측치가 20개, 변수가 4개, 군집이 4개인 경우 조합적 탐색은 연산횟수가 $3.6 \times 10^{12}$에 달하고, 위계적 군집은 2,000~6,000회의 범위인 반면 K평균 군집화 방법은 400T(T는 알고리즘 반복 횟수)회이다. 이 경우 알고리즘을 몇 차례 반복하느냐에 따라 위계적 군집과 K평균 군집의 효율성의 우열이 달라지지만 관측치의 수가 늘어나는 경우, 연산량이 지수적으로 비례하는 위계적 군집에 비해 K평균 군집은 연산량의 증가폭이 급격히 낮아진다. 일반적으로 연산량 측면에서 < 위계적 군집 < K평균 군집 >의 관계가 성립한다.

낮은 연산량이 주는 또다른 이점은 그리드 서치 방식으로 k를 탐색하는 것이 가능하다는 점이다. 위계적 군집이 덴드로그램을 보고 직관적으로 군집의 수를 결정하는 것과 대조적으로 k의 수준을 다르게 하면서 다양한 군집을 시도하면서 실루엣 지수(군집 내 거리와 군집 간 거리의 비율로 계산하는 군집의 유사도 지표), 던 지수(군집 간 최대 거리와 최소 거리의 비율로 계산하는 지표), 갭 통계량(시뮬레이션을 이용하여 산출한 군집 내 거리의 변동이 k의 변화에 따라 얼마나 달라지는지를 추적하는 지표) 등 객관성있는 통계량을 이용하여 최적의 군집 수를 결정할 수 있다. 주관적인 평가를 위해 외생적인 근거가 필요한 위계적 군집과 비교했을 때에, 수리적인 근거가 있는 k값을 찾을 수 있다는 점에서 K평균 군집이 가지는 장점이라고 할 수 있다.

K평균 군집의 단점은 k의 변화에 따른 군집의 변화에 일관성있는 해석이 불가능하다는 점이다. 이 점은 위계적 군집이 가지는 장점이 그대로 K평균 군집의 단점으로 작용하는데, 군집 분석의 결과를 직관적으로 이해할 수 없고, k의 변화에 따라 결과 간에 일관성이 없다는 점은 결과 활용에 큰 어려움으로 작용할 수 있다.

K평균 군집이 탐욕적 알고리즘으로 인해 전역해를 보장하지 않는다는 점과 결합하여 앞선 해석의 어려움은 더욱 가중된다. K평균 군집은 초기 중심점을 랜덤하게 지정하는데, 관측치가 커지는 경우 이러한 임의성 때문에 결과의 일관성이 보장되지 않는다. 실제로 데이터셋을 이용하여 K평균 군집 분석을 진행하는 경우 난수 시드를 고정하지 않으면 결과가 달라지는 사례가 빈번하다. 이론상으로는 데이터 간의 분리가 확연한 경우 초기 중심점을 다르게 잡아도 군집화의 결과는 수렴해야하지만 실제 데이터셋의 분리도가 그렇게 가시적인 사례가 많지 않기 때문이다. 앞서서 설명한 해석불가능성과 결합하여 K평균 군집의 결과물을 어떻게 해석하고 활용할 것인지에 대한 고민이 커질 수 밖에 없는 이유이다. 

#### 7.3.2. K-메도이드medoid 군집 

앞선 K평균 군집에서는 존재하지 않지만 수리적으로 계산할 수 있는 평균값centroid을 중심점으로 두고 군집을 대표하는 지점으로 설정했다. K메도이드 군집은 중심점 대신 중 군집 내의 다른 점들과의 거리가 가장 작은 점을 중앙점medoid으로 정해서 군집화 알고리즘을 작동하는 방식의 분석 기법이다. 

중앙점을 탐색하는 과정이나 군집 간 거리를 평가하는 과정에서 유클리드 거리 이외에 다른 거리체계를 사용할 수 있기 때문에 유클리드 거리를 사용하는 K평균 군집에 비해서 이산형 변수에서도 활용할 수 있다는 장점이 있다. 또, 유클리드 거리체계를 사용하지 않기 때문에 이상치에 민감하지 않아서 군집화의 결과가 강건robust하다. 

반면, K평균 군집에 비해 계산량이 많다. 이는 중심점과 중앙점의 개념차이로 인해 발생하는 차이이기도 한데, 중심점은 관측치에 존재하지 않는 평균값을 임의로 계산하면 정의할 수 있는 반면, 중앙점은 군집 내에 배정된 관측치 각각의 거리를 재평가하는 과정을 거쳐야하기 때문이다. K-메도이드 군집의 시간복잡도는 알고리즘에 따라 다르지만 사장 일반적으로 사용하는 알고리즘에서는 $O(kn^2)$을 따르고, 이보다 탐색속도를 개선한 알고리즘의 경우에는 더 빨라질 수 있다.

---

### 7.4. DBSCAN : 확률이론에 기반한 군집화 방법

밀도에 기반한 군집화 방법은 일정 구간 내에 관측치가 "밀집"한 경우를 군집으로 정의하는 분석 방법이다. 거리를 기반으로 군집을 묶는다는 점에서는 다른 군집화 방법과 동일하지만 관측치 간의 "거리"만으로 동일군집 여부를 판단하는 다른 접근법과는 다르게 밀도기반 군집화는 사전에 정해놓은 "밀도"를 기반으로 동일군집 여부를 판단한다. 그래서 밀도기반 군집은 초모수로 군집의 개수를 정하지 않고 밀도의 요소가 되는 **반경($\epsilon$)**과 **반경 내 관측치 수(minPts : min samples)**를 정해야한다.

DBSCAN은 밀도기반 군집화를 설명하는 교재에서 가장 많이 다루는 분석방법이다. 기본적인 아이디어는 (1) 한 관측치를 기준으로 정해진 반경 내에 정해진 밀도만큼의 다른 관측치가 존재하면 하나의 군집으로 정의하고, (2) 주변에 있는 다른 관측치를 중심으로 하는 군집이 연결되면 하나의 군집으로 묶는 것이다. 이 때문에 DBSCAN은 군집의 개수를 사전에 설정하지 않아도 하나의 군집으로 연결되지 않는 군집은 별도의 군집으로 정의한다.

DBSCAN의 알고리즘은 아래와 같다.

```
1. 임의의 관측치를 중심으로 정해진 반경 내에 있는 다른 관측치의 개수를 세고

    1-1. 반경 내 관측치의 수가 임계치를 넘어가는 경우 군집 번호를 부여한 뒤

    1-2. 군집에 속해있는 다른 관측치를 선정하여 반경 내에 다른 관측치의 수가 임계치를 넘어가는 경우 군집에 추가

    1-3. 1-2단계를 군집 내 모든 관측치에 대해 수행

2. 모든 관측치에 대해 1의 작업을 반복

3. 어느 군집에도 속하지 않은 관측치는 노이즈로 분류
```

[알고리즘 설명 참고 자료](https://devhwi.tistory.com/7)

밀도기반 군집은 2차원 평면 상에서 U자 모양으로 연결된 관측치들을 올바르게 분류하는 문제에 효과적이라고 알려져있다. 실제로 밀도기반 군집을 설명하는 많은 예제에서는 교차된 형태의 말굽모양으로 밀집한 관측치를 분류하는 문제에서 K-평균 군집과 DBSCAN의 결과를 비교해서 보여주는 경우가 많다. 또한 군집의 개수를 미리 정하지 않고 정해진 파라미터 범위 내에서 알고리즘이 자동으로 정해주는 것도 강점이라고 할 수도 있다.

하지만 군집의 개수를 정하는 문제와 밀도를 정하는 문제를 동치라고 생각한다면 DBSCAN이 초모수 선택에서 다른 군집화 기법보다 우월하다고 할 수는 없다. 실제로 DBSCAN을 실행했을 때에 반경과 임계 관측치 수를 다르게 하면 군집화의 결과가 매우 많이 달라진다. 이 점은 DBSCAN의 약점으로도 많이 언급되는데, 고정된 반경과 임계치를 가지고 군집화를 진행하면 군집의 성격에 따라 밀도가 다르면 군집으로 올바르게 인식하지 못하기 때문이다. 이 때문에 가변밀도 데이터 군집화에 더 유리한 HDBSCAN 기법을 이용하기도 한다.

DBSCAN의 강점은 시간 복잡도가 낮고($O(n\log n)$), 알고리즘과 초모수의 성격이 직관적이고 단순해서 이해하기 쉽다는 점에 있다. 또한, 군집에 포함되지 않는 관측치(noise)를 군집에 포함된 개체와 분리할 수 있다는 점도 분석 결과의 타당성을 뒷받침해줄 수 있는 강점이기도 하다.

---

### 7.6. 맺음

군집화를 통한 다변량 데이터 처리는 데이터셋을 상이한 성질의 하위 그룹(subgroup)으로 분리함으로써 후처리를 할 수 있는 가능성을 제시해준다는 점에서 활용 가치가 있다. 하위 그룹의 특징을 바탕으로 분산량이 작은 변수를 제거할 수도 있고, 전체 데이터의 기술적인 특성을 추출할 수 있기 때문이다. 

실제로 데이터 분석 기법 중에는 군집화를 통해 데이터에서 패턴과 분류 기준을 찾아내는 경우도 많고, 군집분석을 전후로 다른 분석기법과 결합하여 데이터 분석의 목적을 달성하는 사례도 많다. 특히, 앞서 설명한 차원축소 기법과 군집화 기법을 결합하는 경우 $n \ll p$ 때문에 생기는 정보량의 불균형을 효과적으로 피할 수 있는 돌파구를 찾아낼 수 있는 강력한 분석 방법이 될 수 있다.

다음주에는 **군집화**와 유사한 방식으로 하나의 데이터를 여러 개의 하위 그룹으로 분리할 수 있는 다양한 분석기법을 알아본다.