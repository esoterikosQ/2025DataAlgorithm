## 12. 신경망 모형

이번 시간에는 신경망 모형의 기초적인 구조를 이해하기 위해 퍼셉트론에 대해 공부하고, 퍼셉트론을 다층으로 쌓은 구조인 바닐라NN에 대해 알아보겠습니다. 퍼셉트론의 개념과 각 노드의 내부 구조, 그리고 바닐라NN의 구조와 작동원리를 이해하는 것은 신경망을 활용한 인공지능의 내부 구조를 이해하는 데에 필수적입니다.

### 12.1. 퍼셉트론 (Perceptron)과 바닐라NN(Vanilla Neural Network)

#### 12.1.1. 퍼셉트론의 개념

퍼셉트론은 1957년 프랭크 로젠블라트(Frank Rosenblatt)가 고안한 알고리즘으로, **인간의 뇌신경 세포(뉴런)를 수학적으로 모델링**한 것입니다. 뉴런은 화학물질로 구성된 신호를 입력으로 받아 화학물질로 구성된 신호를 출력으로 내보냅니다. 이러한 뉴런들은 입력부와 출력부가 서로 얽혀 신경계를 이루며 "뇌"라는 명칭의 장기를 구성합니다.

이러한 인간의 신경망 구조를 컴퓨터 프로그램으로 구현한 것이 바로 퍼셉트론입니다. ([참고자료](https://blog.naver.com/samsjang/220948258166)) 각각의 노드는 신경세포의 구조를 가지고 있고, 입력으로 받은 신호에 가중치와 편향을 이용한 계산값을 구한 후, 이를 활성화 함수를 통해 출력으로 내보냅니다.

최초의 퍼셉트론 구조는 입력층, 출력층으로만 구성된 단층 구조였습니다. 단층 구조의 퍼셉트론이 작동하기 위해서 계산해야하는 값은 가중치와 편향입니다.

---

#### 12.1.2. 퍼셉트론의 내부구조 : 입력, 가중치, 편향

퍼셉트론은 **입력층**에서 데이터를 받습니다. 이 데이터는 일반적인 벡터 형태의 숫자 데이터로 범주형, 연속형 구분을 가리지 않고 받습니다. **가중치**는 각 입력 데이터에 곱해지는 계수값입니다. 가중치가 커지면 해당 입력값이 출력에 미치는 영향이 커집니다. 마지막으로 **편향**은 출력값에 추가되는 상수값입니다. 편향은 출력값의 기준점으로 작용합니다. 

이상의 퍼셉트론의 구성요소를 수식으로 표현하면 다음과 같습니다.

```math
z = w_1x_1 + w_2x_2 + ... + w_nx_n + b
```

위의 식은 n개의 변수를 가지는 다중선형회귀모형과 정확하게 일치하는 구조입니다. 선형회귀와 다른 점은 z값을 그대로 출력하지 않고 별도의 활성화함수를 거친 후 출력한다는 점입니다.

```math
y = h(z)
```

초기의 퍼셉트론은 계단함수를 활성화함수로 사용했습니다. 즉, z값이 특정 임계값을 넘으면 1을 출력하고, 그렇지 않으면 0을 출력하는 방식입니다. 이러한 형태의 퍼셉트론 구조는 일반화 선형회귀모형과 매우 흡사하다고 할 수 있습니다. (활성화함수에 관해서는 후술합니다.)

---

#### 12.1.3. 퍼셉트론의 한계 (XOR 문제)

계단함수를 활성화함수로 가지는 퍼셉트론은 결과값을 0/1로만 가지는 선형 분류기 역할 외에는 수행할 수가 없습니다. 이 경우 AND, OR 연산은 가능하지만 XOR 연산은 불가능합니다. 퍼셉트론을 이용하여 해결할 수 없는 문제의 폭이 명확해지면서 신경망 연구는 한동안 침체기를 겪게 됩니다.

---

#### 12.1.4. 바닐라NN (Vanilla Neural Network) / 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)

신경망 중 가장 기본적인 형태의 신경망으로, 바닐라 아이스크림처럼 가장 기본적인 신경망 구조라는 의미에서 바닐라NN이라고 부릅니다. 좀 더 학술적인 의미가 명확한 용어는 **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**입니다. 이 명칭이 의미하는 바와 같이 바닐라NN은 퍼셉트론을 여러 층으로 쌓은 구조를 가지고 있습니다. 이 때문에 신경세포가 다중적으로 얽혀서 최종적인 출력 신호를 만들어내는 인간의 신경망 구조와 훨씬 더 유사하다고 할 수 있습니다. 

바닐라NN은 입력층과 출력층 사이에 은닉층을 하나 이상 가지는 구조로 되어있습니다.() 이러한 은닉층은 단층 퍼셉트론 구조가 가지는 단순 연산의 한계를 극복하는 데에 결정적인 역할을 합니다. 

XOR 문제를 이용하여 은닉층의 역할을 설명하겠습니다. XOR 문제란, AND 문제(모든 입력이 1일 때에만 출력이 1)나 OR 문제(입력 중 하나라도 1이면 출력이 1)와 달리, 입력값이 서로 다를 때에만 출력이 1이 되는 문제입니다. XOR 문제로 분류되는 형태의 문제는 아래와 같은 논리구조를 가집니다.

*   (0, 0) $\rightarrow$ 0
*   (1, 1) $\rightarrow$ 0
*   (0, 1) $\rightarrow$ 1
*   (1, 0) $\rightarrow$ 1

위 구조를 2차원 평면에 도식화하면 하나의 직선으로는 완벽하게 0과 1을 분리할 수 없는 형태로 그려집니다. 즉, 한 번의 계산과 출력으로 구성되는 선형 분류기로는 완전한 분리가 불가능합니다. 하지만 은닉층을 추가했을 때에 다음과 같은 연산이 일어날 수 있습니다.

1. 은닉층의 노드1이 OR 패턴을 학습하고 노드2가 AND 패턴을 학습합니다. 노드1과 노드2의 출력값을 조합하면 원래의 2차원값과 다른 속성의 2차원값이 생성됩니다.

2. 은닉층에서 생성한 2차원값을 하나의 선형 분류기로 분리합니다.

XOR 문제를 해결하는 과정은 은닉층이 가지는 복합연산의 속성을 단적으로 보여주며, 이러한 속성은 앞으로 다양한 형태의 신경망 모형이 복잡한 문제를 어떻게 풀어가는지를 개념적으로 보여주는 사례입니다. CNN은 은닉층을 하나씩 거쳐갈 때마다 저차원의 특징에서 고차원의 특징을 추출해내는 것으로, RNN은 은닉층 하나하나가 다른 주기의 파동함수를 학습하는 것으로 이해할 수 있습니다. 

이러한 은닉층의 등장은 신경망의 한계를 극복할 수 있는 결정적인 역할을 했고, 제2차 AI 빙하기를 종식하고 현대적인 의미의 인공지능 붐을 일으키는 계기가 되었습니다.

(참고로 제1차 AI 붐은 1956년 다트머스 회의로 인해 시작되었습니다. 다트머스 회의는 "인공지능"이란 개념을 제시한 것으로 평가받는 회의인데, 이 회의를 통해서 인공지능의 개념과 응용분야를 명확히 밝혔고, 그 영향으로 컴퓨터를 이용하여 인간의 지적인 능력을 그대로 모사할 수 있을 것이라는 기대감이 커지면서 인공지능에 대한 관심이 폭발하는 계기가 되었습니다. 인간의 신경망을 모사한 퍼셉트론이 등장하면서 이 시기는 절정에 달했는데 XOR 문제의 벽이 명확해지면서 인공지능에 대한 실망감이 커지고 제1차 AI 빙하기가 찾아옵니다. 이를 극복한 것은 70년대 MIT의 AI Lab에서 인공지능의 코딩을 수행할 수 있는 LISP 언어를 개발하고 이를 수행할 수 있는 컴퓨터를 생산하기 시작하면서부터입니다. 수학적인 문제해결이 아니라 의사와 같은 전문가 집단의 전문적인 판단을 복합적인 if~then 규칙으로 코딩하면 인공지능을 구현할 수 있으며, 이를 뒷받침하는 도구가 LISP 언어와 LISP 머신이었습니다. 그러나 87년 LISP 머신을 생산하던 업계가 도산하면서 두 번째 AI 빙하기가 찾아옵니다. 이를 극복한 것은 제프리 힌튼, 얀 르쿤 등 신경망 연구자들이 다층 퍼셉트론, 역전파 알고리즘, CNN 등을 개발하면서 현대적인 인공지능 연구의 기반을 개척하기 시작했고, 2000년대부터 컴퓨팅 성능이 비약적으로 발전하면서 개념적으로만 존재했던 천문학적 연산 기반의 인공지능 모델이 구현 가능해지면서부터입니다.)

---

### 12.2. 신경망의 구성요소

#### 12.2.1. 신경망의 구조

신경망은 입력층, 은닉층, 출력층으로 이루어집니다. 입력층은 단순히 데이터의 입력을 추상적인 '층'이라는 개념으로 정의한 것으로 실제 신경망에서 특별한 역할을 하지는 않습니다. 실제 신경망의 구조는 은닉층과 출력층으로 이루어지는데, 핵심적인 역할을 하는 것은 은닉층입니다.

**은닉층**의 레이어 개수와 노드 개수는 신경망의 성능을 좌우하는 핵심 초모수(hyperparameter)로 인식될만큼 은닉층의 역할은 중요합니다. 일반적으로 층이 깊어질수록 추상적이고 복잡한 연산을 수행할 수 있는 것으로 알려져있습니다. 노드의 개수는 입력층에 가까울수록 입력층의 차원에 유사하도록 설정하고 출력층에 가까울수록 문제의 복잡도에 맞게 조정하는 것이 일반적입니다. 

또한 은닉층을 구성하는 각각의 노드에서 어떤 방식으로 데이터가 처리되는지에 따라서 전체 신경망의 명칭, 기능, 특수성이 달라집니다. 바닐라 NN에서는 선형회귀모형과 유사한 방식으로 입력값에 가중치를 곱하고 편향을 더한 후 최종적인 계산값을 활성화 함수의 입력값으로 생성한 후 다음 층으로 전달하는 방식의 단순 연산만 일어나지만, 추후에 살펴볼 CNN, RNN, Transformer 등에서는 내부에서 추가적인 연산을 병행합니다. 

**출력층**은 문제의 종류에 따라 뉴런의 개수와 활성화 함수값이 달라집니다. 이를테면 최종적인 연산결과값이 1차원의 스칼라값인 경우 출력층 노드는 1개, 이진 분류의 경우 1개, 다중분류의 경우 클래수의 개수로 이루어지는 것이 일반적입니다. 출력층 내부에서는 은닉층의 각 노드들로부터 전달받은 최종적인 입력값을 연산한 후 활성화함수를 거쳐 최종적인 출력값을 만들어냅니다. 다양한 활성화 함수가 존재하는데, 최근에는 분류문제의 경우 소프트맥스, 회귀문제의 경우 ReLU 함수를 주로 사용합니다.

---

#### 12.2.2. 활성화 함수

활성화 함수는 덧셈과 곱셈으로만 구성된 단순 선형연산만을 수행하는 회귀모형과 신경망의 차이를 가장 단적으로 보여주는 장치입니다. 일반화 선형회귀가 연결함수를 통해 선형회귀에서 포괄하지 못하는 분포의 형태로 모형을 연결하는 것과 마찬가지로, 활성화 함수 역시 비선형성 요소를 포괄할 수 있도록 하는 역할을 합니다.

비선형 함수가 존재하지 않는 신경망 모형의 연산은 아래와 같이 표현할 수 있습니다.

```math
\begin{align}
\text{1층 : } &y_1 = a_1 x + a_0 \\
\text{2층 : } &y_2 = b_1 y_1 + b_0 \\
\text{3층에서 받는 값 : } &y_3 = c_1 b_1 a_1 x + (c_1 b_1 a_0 + c_1 b_0 + c_0)
\end{align}
```

이러한 방식으로 층을 아무리 반복해도 최종적으로 계산한 값은 여전히 x를 중심으로 가중치와 편향값만 달라진 선형 모형에 불과합니다. 반면, 활성화 함수 중 가장 단순한 형태인 계단 함수를 사용하여 각 층의 연산 결과값을 처리하면 다음과 같은 형태가 됩니다.

```math
\begin{align}
\text{1층 : } &y_1 = h(a_1 x + a_0) \\
\text{2층 : } &y_2 = h(b_1 y_1 + b_0) \\
\text{3층에서 받는 값 : } &y_3 = h(c_1 y_2 + c_0)
\end{align}
``` 

즉, 선형함수를 이용하여 각 노드의 연산을 처리했지만 마지막 단계에서 h()로 표현한 활성호함수를 거치기 때문에 최종적인 노드의 출력값은 0 또는 1로 변환됩니다. 이 과정을 거치기 때문에 최종적인 출력층에서는 최초 입력값과 단순 선형관계가 아닌 출력값을 생성하고, 최종적으로 출력하는 값 역시 치역에 해당하는 범위 내의 값을 유지할 수 있게 됩니다.

일반화 선형회귀모형의 연결함수가 여러 종류 있는 것과 동일하게 신경망 모형의 활성화 함수 역시 여러 종류가 있습니다.

##### 12.2.2.1. 계단 함수 (Step Function)

가장 단순한 형태의 활성화 함수로, 초기의 퍼셉트론에서는 계단함수를 활성화 함수로 사용했습니다. 계단 함수는 다음과 같은 수식으로 표현할 수 있습니다. [계단함수의 형태](https://namu.wiki/w/헤비사이드%20계단함수)

```math
h(z) = \begin{cases} 0 & (z \le 0) \\ 1 & (z > 0) \end{cases}
```

계단함수는 이진분류 문제에서 효과적인 출력값을 생성할 수 있다는 장점이 있지만 분기점에서 미분이 불가능하고, 분기점 외의 다른 모든 구간에서 미분값이 0이기 때문에 학습을 통해 가중치를 업데이트할 수 없다는 단점이 있습니다. 이로 인해 계단함수는 현재 신경망 모형에서 거의 사용되지 않습니다.

##### 12.2.2.2. 시그모이드 함수 (Sigmoid Function)

시그모이드 함수는 0과 1 사이의 값을 가지는 매끄러운 S자 곡선 형태의 함수로, 다음과 같은 수식으로 표현할 수 있습니다.

```math
\sigma(z) = \frac{1}{1+e^{-z}}
```

계단함수와 동일하게 이진분류가 가능하고 모든 점에서 미분이 가능하기 때문에 역전파 방식으로 가중치를 업데이트할 수 있다는 장점이 있습니다. 반면, 입력값의 절대값이 커질수록 기울기값이 0에 가까워집니다. 이 때문에 은닉층이 많은 복잡한 모형에서는 특정 데이터가 거치는 특정 노드의 기울기값이 사라지는 문제가 발생하기 때문에 최근에는 잘 사용되지 않습니다. 

##### 12.2.2.3. 하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent, tanh)

시그모이드 함수와 비슷하지만 치역이 (-1, 1)인 함수입니다. 함수의 중심점이 0이기 때문에 시그모이드보다 학습 효율이 좋은 것으로 알려져있지만, 시그모이드의 문제점인 기울기 소실 문제도 공유하고 있습니다.

```math
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
```

##### 12.2.2.4. ReLU (Rectified Linear Unit)

오늘날 신경망 모형에서 거의 표준에 해당하는 위상을 차지하고 있는 활성화 함수입니다. [ReLU 함수의 형태](https://gooopy.tistory.com/55)

```math
f(z) = \max(0, z)
```

ReLU 함수는 시그모이드 계열의 함수와 비교해서 두 가지 장점을 가집니다. 첫째로, 계산의 효율성이 좋습니다. 입력값이 커질수록 미분값이 작아지는 시그모이드 계열 함수와 반대로 ReLU 함수는 양수 구간에서 미분값이 항상 1이기 때문에 미분을 거치는 역전파 과정에서도 연산량이 적고, 양수 구간에서는 입력값을 변환하지 않고 반환하기 때문에 학습을 통해 가중치를 수정하는 속도도 월등합니다. 두번째로, 기울기 소실 문제를 해결할 수 있습니다. ReLU 함수는 양수 구간에서 미분값이 항상 1이기 때문에 신경망의 은닉층이 깊어지는 경우에도 기울기가 사라지지 않습니다. 

반면, ReLU 함수는 입력값이 0 이하인 구간에서 고정적으로 0의 값을 가지기 때문에 우연한 기회로 모든 노드의 연산값이 0 이하로 낮아지는 노드가 생기면 해당 노드의 출력값은 이후의 결과에 영향을 미치지 못합니다. 이로 인해 특정 노드의 역할을 상실하는 문제가 발생하는데, 이를 Dying ReLU라고 합니다. 이를 해결하기 위해 음수 구간에서도 0이 아닌 작은 값을 가지는 형태의 변형된 ReLU 함수들이 제안되고 있습니다.


##### 12.2.2.5. 소프트맥스 함수 (Softmax Function)

이진 분류가 아닌 다중 분류 문제의 출력층에서 사용하는 활성화 함수입니다. 입력받은 값들을 0과 1 사이의 값으로 정규화하여 출력하며, 출력값들의 총합은 항상 1이 됩니다.

$n$개의 출력 뉴런이 있을 때, $k$번째 뉴런의 출력 $y_k$는 다음과 같습니다.

```math
y_k = \frac{e^{z_k}}{\sum_{i=1}^{n} e^{z_i}}
```

전체 출력값 중에서 가장 큰 값을 가지는 노드의 범주가 최종적인 예측값이 되는 구조입니다. 소프트맥스 함수는 현재 대형 LLM 모델의 출력층에서 표준적으로 사용되고 있습니다. 차원이 수천에서 수만에 달하는 범주 중에서 출력값을 계산하는 구조로 이루어져있습니다. 이 때문에 소프트맥스 함수의 단점인 미세한 출력값의 차이가 결과를 좌우하는 현상이 발생할 수 있습니다. 이 때문에 소프트맥스 함수는 현재 LLM 모델의 출력층에서 개선의 여지가 있는 부분으로 지적되며, 다양한 변형 함수들이 제안되고 있습니다.

---

### 12.2.3. 순전파와 역전파 (Forward & Backward Propagation)

현대의 신경망 구조에서 가중치와 편향값을 학습하는 방법은 순전파와 역전파 알고리즘입니다. 특히, 역전파 알고리즘은 다층 신경망에서 가중치를 효율적으로 계산하는 방법으로 제안되면서 신경망 연구에 획기적인 전환점을 가져왔습니다.


#### 12.2.3.1. 순전파 (Forward Propagation)

순전파는 입력 데이터가 신경망을 통과하여 출력값을 계산하는 과정입니다. 데이터는 신경망 구조를 거치면서 최종적인 출력값으로 변화합니다. 

#### 12.2.3.2. 역전파 (Backpropagation)

순전파 과정에서 사용하는 가중치와 편향값은 임의로 설정한 값을 이용합니다. 이렇게 계산한 최종적인 출력값을 실제값과 비교하여 손실값을 계산하는데, 손실값의 크기에 따라 각 층의 각 노드가 가지는 가중치와 편향값을 보정합니다. 이 과정을 역전파라고 합니다.

여기서 **손실함수**라는 개념이 등장합니다. 손실함수는 순전파를 통해 최종적으로 계산한 값이 실제값과 얼마나 다른지를 계산하여 수치화하는 함수입니다. 이 손실함수 값을 최소화하는 방향으로 가중치와 편향값의 보정이 이루어집니다. 일반적으로 회귀문제에서는 평균제곱오차(MSE, Mean Squared Error), 분류문제에서는 교차엔트로피오차(CEE, Cross-Entropy Error)를 손실함수로 사용합니다.

역전파 과정은 손실함수값을 이용하여 각 단계별 가중치와 편향값을 조정하는 과정입니다. 이를 위해 손실함수 $L$에 대한 $w$의 미분값, 즉 $\frac{\partial L}{\partial w}$라는 개념이 등장합니다. 즉, 가중치가 변화할 때에 손실함수값이 얼마나 변하는지를 기준으로 가중치를 조정했을 때에 손실함수값이 어떻게 줄어들지를 예측할 수 있고, 이를 바탕으로 가중치를 조정합니다. 

하지만 손실함수는 직접적으로 기울기의 영향을 받는 값이 아닙니다. $L$은 활성화함수를 거친 $\hat{y}$에 의존하고, $\hat{y}$는 $z$에 의존하고, $z$는 $w$에 의존합니다. 이러한 연쇄적인 의존관계로부터 가중치와 손실함수의 미분값을 구하기 위해 연쇄 법칙(Chain Rule)을 사용하여 최종적인 가중치의 변동분을 계산합니다.

출력층의 가중치를 조정하고 나면 출력층의 입력값을 역전파함으로써 은닉층의 노드의 가중치와 편향 역시 계산이 가능합니다. 

신경망의 전체 가중치와 편향값 조정은 이러한 순전파와 역전파 과정을 반복함으로써 최종적으로 실제값과 가장 차이가 적은 출력값을 만들어낼 수 있도록 이루어집니다. 이러한 과정을 신경망 "훈련" 또는 "학습"이라고 부릅니다.  

전체 학습 과정에서 데이터를 훈련용 데이터와 검증용 데이터로 나누는데, 훈련용 데이터 전체를 이용하여 한 차례 학습을 종료하는 과정을 에폭(epoch)이라고 부릅니다. 일반적으로 모델 하나를 학습시킬 때에는 수 백에서 수 십만 에폭을 반복합니다. 에폭의 반복 횟수를 정하는 것 역시 초모수에 해당하기 때문에 일반적인 결정의 기준은 없지만, 가장 일반적으로 사용하는 방법은 검증용 데이터와 훈련용 데이터의 손실함수값의 변화가 일정 수준 이하로 줄어든 후 더이상 줄어들지 않는 시점에서 강제로 학습을 종료하는 방식입니다. 이를 **조기 종료(Early Stopping)**라고 부릅니다.

(이상의 신경망 학습 과정을 곱씹어보면, 곱셈과 덧셈의 연산이 무수히 많은 횟수 반복되는 것을 알 수 있습니다. 이 때문에 신경망을 학습하기 위해서는 단순연산을 매우 빠르게 수행할 수 있는 하드웨어가 필요한데, 이 연산 과정이 컴퓨터 그래픽 처리에서 무수히 많은 선을 생성하는 방식과 흡사합니다. 이 점에 착안하여 엔비디아에서는 2000년대 초반부터 GPU를 단순히 그래픽 처리에만 활용하는 것이 아니라 단순 연산을 병렬적으로 처리할 수 있는 소프트웨어와 개발도구를 제공하면서 신경망 학습용 하드웨어로 GPU 시장을 개척하기 시작했습니다. 오늘날 오픈소스 진영에서 사용하는 신경망의 개발 프레임워크는 엔비디아사에서 이때부터 제공하기 시작한 개발환경인 쿠다(CUDA)를 기반으로 하고 있습니다.)

---

#### 12.3.4. 기울기 소실과 포화 문제

은닉층을 포함하는 신경망르 학습시킬 때에 발생하는 여러 문제들 중에서 초기 단계에서 가장 쉽게 접할 수 있는 문제가 바로 **기울기 소실(Vanishing Gradient)**과 **포화문제(saturation problem)**입니다. 

기울기 소실 문제는 역전파 과정에서 출력층에 가까운 특정 노드의 기울기가 0에 가까워지면 입력층에 가까운 선행층의 선행노드의 가중치가 거의 업데이트되지 않는 현상입니다. 연쇄법칙을 이용해서 기울기 갱신값을 계산하는데, 특정 성분의 미분값이 0에 가까워지면 곱셈으로 이루어진 전체 미분값 역시 0에 가까워지는 현상으로 이해하시면 됩니다. (자세한 수식은 수업시간에 다루도록 하겠습니다.)

기울기 소실이 발생하면 입력층에 가까운 층의 가중치 업데이트가 거의 이루어지지 않기 때문에 전체 신경망의 학습속도가 매우 느려지거나 아예 멈추는 현상이 발생합니다. 이 때문에, 신경망을 설계할 때에 기울기 소실이 일어나지 않게 하는 여러 가지 보완책을 병행합니다. 가장 널리 사용하는 방식은 은닉층의 활성화함수에 ReLU 계열의 함수를 사용하는 것입니다. 최근에는 거의 모든 신경망의 은닉층 노드에서는 활성화함수로 ReLU 계열의 함수를 사용하고 있다고 보셔도 무방합니다. 

포화 문제란 뉴런의 출력값이 활성화 함수의 극단값에 머물러서 변화하지 않기 때문에 신경망의 학습이 제대로 이루어지지 않는 문제를 의미합니다. 시그모이드 함수나 tanh 함수의 경우 입력값이 극단으로 향할수록 출력값이 0, 1, -1로 수렴하는 특성이 있습니다. 이 값으로 은닉층 노드의 출력값이 수렴하는 경우 역전파 과정에서 기울기가 0에 가까워지기 때문에 기울기 소실이 발생할 가능성이 높아집니다. ReLU 함수를 사용하는 경우 입력값이 음수의 영역으로 들어갔을 때에 노드가 죽는 문제 역시 포화 문제의 일종으로 볼 수 있습니다. 

포화문제를 방지하기 위해 ReLU 계열의 활성화함수의 변형된 형태인 Leaky ReLU, Parametric ReLU 등을 사용합니다. 또한, 가중치가 지역해(local minima)에서 포화 문제가 생기는 문제를 방지하기 위해서 가중치를 0에 극단적으로 가까운 값을 일괄적으로 부여하지 않고 적절한 분산을 가지는 무작위 값으로 초기화하거나 정규화하는 방식을 사용하기도 합니다. 

또한, 학습률을 자동으로 조정하는 옵티마이저를 사용하여 기울기 소실과 포화 문제를 완화하는 방법도 있습니다. Adam 옵티마이저는 이러한 문제를 완화하는 데에 매우 효과적인 것으로 알려져 있어서 오늘날 신경망 모형을 설계할 때에는 학습률을 수동으로 조정하지 않고 거의 항상 Adam 옵티마이저를 사용합니다.

---

### 12.5. 맺음

신경망은 오늘날 예측의 영역에서 범접할 수 없는 압도적인 성능을 보여주는 분석 기법입니다. 특히, 정형화 데이터뿐만 아니라 자연어, 이미지, 음성 등 비정형 데이터를 활용한 실전 기법에서도 신경망의 예측 성능을 추종할 수 있는 다른 분석방법은 존재하지 않기 때문에 거의 모든 영역에서 문제 해결을 위해 신경망에 맞는 방식으로 문제를 수정하거나 조정하는 사례가 있을 정도입니다. 이러한 신경망의 기본적인 구조를 확인하고, 작동원리를 이해하는 것은 실제 신경망을 활용하는 분석을 진행할 때에 필수적으로 알아야 하는 내용들입니다. 다음 시간에는 이러한 기본적인 신경망의 원리를 변형하여 이미지 분석, 시계열 데이터 분석에 특화시키거나 대규모 데이터 학습에 특화된 다양한 변형 모형에 대해서 알아보겠습니다.